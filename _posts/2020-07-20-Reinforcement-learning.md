---
layout: post
title:  "[D] Reinforcement Learning Algorithms"
date:   2020-07-20 11:00:00 +0800
categories: RL
tags: 强化学习算法 D
author: Xlp
---
* content
{:toc}


## 简介

> 本部分包含对各种经典的强化学习算法的总结，包括但不限于DQN、DDPG、PPO、TRPO等算法。





## Deep Q-Learning
### 背景
DQN算法的提出是在以CNN为代表的深度学习在图像处理上取得显著效果之后。
在DQN提出之前，一直以来强化学习的主要方式表现为表格法和值函数近似法。
值函数近似又可分为线性值函数近似和非线性值函数近似。
深度神经网络属于非线性函数，且在图像处理上表现出了强大的拟合能力。
因此，强化学习与深度学习的结合的产物--深度强化学习，是必然的。

表格法受限于其对状态和动作空间大小的限制：当状态空间或动作空间变得极大时，会造成维度灾难；
而当状态空间或者动作空间无限时，表格法就无能为力了（直接表示是不可能的，只能通过其他类似于重构的方法来处理）。

值函数近似可以打破这种限制，它无所谓状态与动作的维度限制，若值函数拟合的好的话，那么对于任何状态与动作都可以通过表达式来计算出一个无限接近于真实值的近似值。
将值函数得到的值称为近似值是因为我们对于真实值的实际分布是什么样子的，近似值函数的本质就是通过一个函数来近似 Q 值 或者 V 值。

既然需要利用深度神经网络强大的拟合能力和表征能力，那么深度学习与强化学习的融合有哪些障碍呢？
- 标签（label）
  - 普通深度学习属于监督学习，依赖于大量带label的样本数据；
  - 而强化学习只有奖励可以算是标签，但是在强化学习中奖励具有稀疏性、带有噪音且具有延迟的特性；
- 数据的独立性
  - 深度学习用来训练的样本是要求相互独立的；
  - 而强化学习的样本是具有高度相关性的，具有鲜明的先后顺序；
- 数据分布的平稳性
  - 深度学习要求数据服从平稳分布，即在训练过程中，数据服从的分布是不变的；
  - 而强化学习的数据分布会随着算法学习到的新的行为策略改变而改变；

那么，DQN是如何克服这些障碍的呢？
- Replay Buffer
- Target Network

### 创新

> 在DQN之前，大多数强化学习问题都需要研究任务的领域知识（以提取特征等），同时需要解决同一序列内样本之间相互关联的问题，以及Q-learning算法中价值估计与更新值相互关联的问题。
> 作者通过 Replay Buffer、均匀随机采样、深度神经网络、目标网络等一系列设计和改进使得模型在Atari游戏上达到了优秀的效果，也为开启了强化学习和深度学习的结合之路。

#### Replay Buffer
Q-learning 方法基于当前策略进行交互和改进，更像是一种在线学习（Online Learning）的方法：每一次模型利用交互生成的数据进行学习，学习后的样本被直接丢弃。
但是当使用神经网络等机器学习模型代替表格式模型，依然采用这样的在线学习时，就会遇到问题：
- 交互得到的序列存在一定的相关性：交互序列中的状态、动作存在着一定的相关性，而对于基于最大似然法的机器学习模型来说，我们有一个重要的假设：训练样本是独立同分布的，一旦这个假设不成立，那么模型的
效果就会大打折扣；
- 交互数据的使用效率：采用梯度下降法进行模型更新时，模型训练往往需要经过多轮迭代才能收敛，每一次迭代需要使用一定数量的样本计算梯度，如果每次计算的样本在计算一次梯度之后就被丢弃，那么我们需要花费更多
的时间与环境交互收集样本（对于一些一次交互需要花费较长时间的任务来说，这是致命的）；

为了解决这个问题，作者提出了 Replay Buffer 这样的数据结构：Replay Buffer 中保存的是 agent 与 environment 交互产生的 `(s, a, r, s')` 这样的 transition。
它包含收集样本和采样样本两个过程：收集样本时按照时间先后顺序存入结构中，如果 Replay Buffer 存满了的话，就会从开始进行旧数据的覆盖（原则上是优先覆盖时间越久远的样本）；
采样样本的过程表现为在每次训练模型之前，在 Replay Buffer 中均匀的随机采样一批样本进行学习。

那么为什么使用均匀采用呢？它有什么好处呢？   
前面提到我们交互得到的序列在时间维度上存在一定的相关性。
我们希望学习得到的 **值函数** 能够表示 **在当前状态-动作下的长期收益的期望**，然而每一次交互得到的序列，只能代表当前状态-动作下的一次采用轨迹，并不能代表所有可能的轨迹，这样估计的结果就和期望的结果存在一定的差距。
随着交互时间不断拉长，这个差距的累积就会越来越大。
如果完全使用序列的估计值进行训练，某一轮训练时模型会朝着一个序列的估计训练，另一轮训练又会朝着另一个序列的估计训练，那么模型就很容易产生较大的波动。
采用均匀采样后，每次训练的样本通常来自多次交互序列，这样单一序列的波动就被减轻很多，训练效果也就稳定了很多。
同时，一份样本也可以被多次训练，提高了样本的利用率。

事实上，总的来说，均匀的随机采样可以打破数据之间的关联性，间接满足训练所需求的独立同分布的特性，同分布的解释为：将所有的不同分布的历史数据放入 Replay Buffer 中，那么所有的不同分布将中和成为一个新的分布，
且所有数据均满足这个新的分布。

#### Target Network

> Target Network 通过在一段时间内固定网络的参数，使得Q-learning方法的目标价值能够得到一定的固定，间接使得模型获得一定的稳定性。

模型在训练过程中不稳定的另一个原因来自算法本身：从 Q-learning 的计算公式可以看出，算法可以分为如下两个步骤
- 计算当前状态-动作下的价值目标值：$\Delta q(s,a) = r(s') + max_{a'} q^{T-1}(s', a')$；
- 网络模型的更新：$q^{T}(s,a) = q^{T-1}(s,a) + \frac{1}{N} [\Delta q(s,a) - q^{T-1}(s,a)]$；

可以看出模型通过当前时刻的回报和下一时刻的价值估计进行更新，这既像一场自导自演的电影，又像一场既当运动员又当裁判员的比赛。
这里存在一些隐患，前面提到数据样本差异可能造成一定的波动，由于数据本身存在着不稳定性，每一轮迭代都可能产生一些波动，如果按照上面的计算公式，这些波动会立刻反映到下一个迭代的计算中，这样我们就很难得到一个平稳的模型。
为了减轻相关问题到来的影响，我们要尽可能地将两个部分解藕。

为了能够缓解上面提到的波动性的问题，引入了目标网络，该模型与另一个结构完全一样，而原本的模型被称为表现模型（Behavior Network）。
两个模型的训练过程如下：
- 在训练开始时，两个模型使用完全相同的参数；
- 在训练过程中，Behavior Network 负责与环境进行交互，得到交互样本；
- 在学习过程中，Q-Learning 的目标价值是由 Target Network 计算得到，然后用它和 Behavior Network 的估计值进行比较得出 TD Error 并更新 Behavior Network；
- 每当训练完成一定轮数的迭代，Behavior Network 模型的参数就会同步给 Target Network，这样就可以进行下一阶段的学习；

通过使用 Target Network，计算目标价值的模型在一段时间内将被固定，这样就可以减轻模型的波动性。

### Trick
DQN中使用的技巧有：
- Replay Buffer：提高样本利用率，打破数据的关联性以及非平稳分布的特性，实现数据的独立同分布；
- Target Network：在一段时间内固定目标值，减轻模型的波动性；
- 值网络的架构：模型的架构很灵活，原论文中模型的输入为 `(s,a)` 输出单个 `Q(s,a)`，而 deepmind 在 DQN 的实现中，则是使用 `(s)` 作为模型输入，输出 $Q(s,a_1), Q(s, a_2), \cdots, Q(s, a_n)$；

### 网络结构图

<div align="center"><img src="../../../../image/DQN/DQN网络结构图.jpeg"></div>

### 算法

```
DQN 算法
--------------------------------------------------------
Input
01. 初始化容量为 N 的 Replay Buffer D
02. 初始化状态-行为价值模型 Q 和参数 theta
03. 初始化 Target Network Q' 和参数 theta' = theta
Satrt
04. for episode = 1, M do
05.     初始化环境，得到初始状态 s_1，并预处理得到 \psi_1 = \psi_1(s_1) // 预处理指的是对图像大小的处理，论文中是将 210 x 160 的图像转换为了 84 x 84 的图像
06.     fot t = 1, T do
07.         以 epsilon 的概率随机选择一个动作 a_t，或者以 1 - epsilon 的概率根据模型选择当前最优的动作 a_t = max_a Q^{*}(\psi(s_t), a; theta)
08.         执行动作 a_t，得到新一轮的状态 s_{t+1} 和 回报 r_{t+1}
09.         预处理得到 \psi_{t+1} = \psi_{t+1}(s_{t+1})
10.         将 {\psi_{t}, a_t, r_{t+1}, \psi_{t+1}} 存储到 D 中
11.         从 D 中采样得到一批样本 (\psi_{j}, a_j, r_{j+1}, \psi_{j+1})
12.         计算 y_i = r_{j+1} for terminal \psi_{j+1}  r_{j+1} + gamma * max_a' Q(\psi_{j+1}, a'; theta')
13.         根据目标函数 (y_i - Q(\psi{j}, a_j; theta))^2 进行梯度下降，更新参数 theta
14.         每个 C 轮完成目标网络的参数更新 theta' = theta
15.     end for
16. end for
-------------------------------------------------------
```

### 延伸
#### Double DQN
> 为了减少模型的波动性，Double DQN 可以使用不同的模型完成最优行动的选择和价值估计两个工作，使价值估计的波动降低。
> 换句话说，DDQN 就是在 nature DQN 的基础上，通过解藕目标 Q 值动作的选择和目标 Q 值的估计这两步，来缓解过度估计的问题，降低值估计的波动性。

在 Nature DQN 中，加入了 Target Network 来使得目标值在一段时间内被固定，以使模型获得一定的稳定性。
虽然这个方法提升了模型的稳定性，但是它并没有解决另外一个问题：Q-learning 对价值估计过高的问题。  
所谓的过度估计就是算法使得模型存在很大的偏差。
具体描述为：因为 Q-learning 在计算时利用了下一时刻的最优值，所以它通常在计算时给出了一个状态-行动的估计上限，由于训练过程中模型并不够稳定，因此对上限的估计存在一定的偏差；
如果偏差时一致的，也就是说，每个行动都拥有相近的偏差，那么偏差对模型效果的影响相对较小；如果偏差是不一致的，那么这个偏差会造成模型对行动优劣的判断偏差，这样就会影响模型的效果。

为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。

我们已经知道 Target Network 求解价值目标值时的公式：

$$
\begin{align}
y_j = r_{j+1} + \gamma max_{a'} Q(s_{j+1}, a'; \theta^{-}) \tag{1}
\end{align}
$$

其中 $\theta^{-}$ 表示 Target Network 的参数。

将式（1）进一步展开，

$$
\begin{align}
y_j = r_{j+1} + \gamma Q(s_{j+1}, argmax_{a'} Q(s_{j+1}, a'; \theta^{-}); \theta^{-}) \tag{2}
\end{align}
$$

通过式（2）我们可以发现，在采用 target network 之后，模型在选择最优动作和计算目标值时依然使用了相同的参数模型，这样必然会造成对价值的过高估计。

为了尽可能的减少过高估计的影响，一个简单的办法就是把「选择最优动作」和「估计最优动作」两部分的工作分离，可以使用 behavior network 来完成最优动作的选择，这样就可以得到

$$
\begin{align}
y_j = r_{j+1} + \gamma Q(s_{j+1}, argmax_{a'} Q(s_{j+1}, a'; \theta); \theta^{-}) \tag{3}
\end{align}
$$

这样算法的三个环节模型的使用如下：
- 采样阶段：Behavior Network $Q(\theta)$
- 选择最优动作阶段：Behavior Network $Q(\theta)$
- 计算目标价值 $y$ 阶段：Target Network $Q(\theta^{-})$

#### Dueling DQN

#### RainBow

#### Distributional DQN


















































