---
layout: post
title:  "[D] Reinforcement Learning Algorithms"
date:   2020-07-20 11:00:00 +0800
categories: RL
tags: 强化学习算法 D
author: Xlp
---
* content
{:toc}


## 简介

> 本部分包含对各种经典的强化学习算法的总结，包括但不限于DQN、DDPG、PPO、TRPO等算法。

## Deep Q-Learning
### 背景
DQN算法的提出是在以CNN为代表的深度学习在图像处理上取得显著效果之后。
在DQN提出之前，一直以来强化学习的主要方式表现为表格法和值函数近似法。
值函数近似又可分为线性值函数近似和非线性值函数近似。
深度神经网络属于非线性函数，且在图像处理上表现出了强大的拟合能力。
因此，强化学习与深度学习的结合的产物--深度强化学习，是必然的。

表格法受限于其对状态和动作空间大小的限制：当状态空间或动作空间变得极大时，会造成维度灾难；
而当状态空间或者动作空间无限时，表格法就无能为力了（直接表示是不可能的，只能通过其他类似于重构的方法来处理）。

值函数近似可以打破这种限制，它无所谓状态与动作的维度限制，若值函数拟合的好的话，那么对于任何状态与动作都可以通过表达式来计算出一个无限接近于真实值的近似值。
将值函数得到的值称为近似值是因为我们对于真实值的实际分布是什么样子的，近似值函数的本质就是通过一个函数来近似 Q 值 或者 V 值。

既然需要利用深度神经网络强大的拟合能力和表征能力，那么深度学习与强化学习的融合有哪些障碍呢？
- 标签（label）
  - 普通深度学习属于监督学习，依赖于大量带label的样本数据；
  - 而强化学习只有奖励可以算是标签，但是在强化学习中奖励具有稀疏性、带有噪音且具有延迟的特性；
- 数据的独立性
  - 深度学习用来训练的样本是要求相互独立的；
  - 而强化学习的样本是具有高度相关性的，具有鲜明的先后顺序；
- 数据分布的平稳性
  - 深度学习要求数据服从平稳分布，即在训练过程中，数据服从的分布是不变的；
  - 而强化学习的数据分布会随着算法学习到的新的行为策略改变而改变；

那么，DQN是如何克服这些障碍的呢？
- Replay Buffer
- Target Network

### 改进
#### Replay Buffer
Q-learning 方法基于当前策略进行交互和改进，更像是一种在线学习（Online Learning）的方法：每一次模型利用交互生成的数据进行学习，学习后的样本被直接丢弃。
但是当使用神经网络等机器学习模型代替表格式模型，依然采用这样的在线学习时，就会遇到问题：
- 交互得到的序列存在一定的相关性：交互序列中的状态、动作存在着一定的相关性，而对于基于最大似然法的机器学习模型来说，我们有一个重要的假设：训练样本是独立同分布的，一旦这个假设不成立，那么模型的
效果就会大打折扣；
- 交互数据的使用效率：采用梯度下降法进行模型更新时，模型训练往往需要经过多轮迭代才能收敛，每一次迭代需要使用一定数量的样本计算梯度，如果每次计算的样本在计算一次梯度之后就被丢弃，那么我们需要花费更多
的时间与环境交互收集样本（对于一些一次交互需要花费较长时间的任务来说，这是致命的）；

为了解决这个问题，作者提出了 Replay Buffer 这样的数据结构：Replay Buffer 中保存的是 agent 与 environment 交互产生的 `(s, a, r, s')` 这样的 transition。
它包含收集样本和采样样本两个过程：收集样本时按照时间先后顺序存入结构中，如果 Replay Buffer 存满了的话，就会从开始进行旧数据的覆盖（原则上是优先覆盖时间越久远的样本）；
采样样本的过程表现为在每次训练模型之前，在 Replay Buffer 中均匀的随机采样一批样本进行学习。

那么为什么使用均匀采用呢？它有什么好处呢？

### Trick

### 算法

### 延伸
#### Double DQN

#### Dueling DQN

#### RainBow

#### Distributional DQN


















































