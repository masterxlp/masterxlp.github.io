---
layout: post
title:  "[D] Neural Network"
date:   2020-07-17 15:00:00 +0800
categories: ML DL NN
tags: 神经网络类总结 D
author: Xlp
---
* content
{:toc}


## 简介

> 本部分包含对各类神经网络中各类问题的总结，包括但不限于背景、原理、特性、差异等。





## 卷积神经网络

> 卷积神经网络（Convolutional Neural Networks, CNN）是一种前馈神经网络，其特点是每层的神经元只响应前一层局部区域内的神经元。

### 背景

> 参考自：
>> [1] [easyai](https://easyai.tech/ai-definition/cnn/)

在 CNN 被提出之前，类似于图像这种的网格结构的数据对于 AI 来说是一个难题，这主要是因为：
- 图像需要处理的数据量太大，导致成本很高，效率很低；
- 图像在数字化的过程中很难保留原有的特征，难以处理图像不变性，导致图像处理的准确率不高；

由于图像是由像素构成的，每个像素又是由颜色（RGB）构成的。
因此，对于一张 $1000 \times 1000$ 像素的图片，需要处理（MLP网络）的参数达到三百万个：$1,000 \times 1,000 \times 3 = 3,000,000$。
这么大的参数量处理起来是非常消耗资源的，而且这还是只是一张不算太大的图片。

卷积神经网络，解决的第一个问题就是：**将复杂问题简化**，即将大量参数降维成少量参数，再做处理。
重要的是：在大部分场景下，降维并不会影结果。
例如，1000像素的图片缩小成200像素并不影响肉眼识别出图片中的是一只猫还是一只狗，机器也是如此

图像数字化的传统方式，简化版如下：

<div align="center"><img src="../../../../image/图像数字化经典方式.png" width="70%" height="70%"></div>

假设有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达（这是由于在普通的前馈神经网络中，只能处理线性的数据，所以需要将网格数据拉平后才能进行进一步的处理）。
但是从视觉的角度来看，图像的内容（本质）并没有发生变化（从空间结构上看，相对位置的信息并没有发生改变），改变的只是位置（像素或者球的位置发生了变化）。
所以，当我们移动图像中的物体（即平移、旋转等）时，用传统的方式得到的参数会有很大差异（因为它破坏了图像的结构信息），这不符合图像处理的要求。

卷积神经网络，解决的第二个问题是：用类似视觉的方式保留图像的特征，即当图像做旋转、平移（变换位置）时，它依然可以有效的识别出类似的图像。

### 原理
#### 卷积层
卷积层时卷积神经网络的基本结构，它是由多个卷积核组合形成的，每个卷积核痛输入数据卷积运算，形成新的特征“图”。
- 卷积核（Convilutional kernel），也被称为滤波器（filter）；
- 卷积核的“矩阵”值，表示为卷积层的参数，其初始值是随机生成的，可通过反向传播更新；
- 卷积核的数量决定了输出的通道数（该层的输出通道数 = 下一层的输入通道数）；
  - 卷积核的数目为超参数，需要用户设定；
  - 一般为 64、128、256；
- 卷积核的形状：[kernel weight, kernel height, input channel]；
- 卷积核的大小是由用户定义的：
  - 奇偶选择：一般奇数，满足中心对称；
  - 大小选择：根据输入数据、图像的特征来决定，一般为 $3 \times 3$、$5 \times 5$、$7 \times 7$；
  - 厚度选择：与输入数据一致，即卷积核的通道数 = 输入通道数；
  - 步长（stride）：对输入特征图的扫描间隔；
  - 边界扩充（padding）：在卷积计算过程中，为了允许边界上的数据也能作为中心参与卷积运算，将边界进行零扩充；
    - 目的：确保卷积后特征图尺度一致；
    - 方法：卷积核的宽度 $2i + 1$，则添加pad宽度为 $i$；
- 每一个卷积核都会与整个输入图像做卷积生成一张特定的特征图，表达了对某一个特征的提取；

卷积层的参数计算：$param\ number = input\ channels \times kernel\ weight \times kernel\ height \times output\ channels (e.g.\ kernel\ number)$。

卷积层的作用：卷积层就是通过卷积核来提取特征的。

反向传播的意义：因为初始的卷积核的值（卷积层的参数）是随机设定的，我们可以根据前向传播的预测结果，进行误差分析，不断的修改卷积核的值，使得卷积核可以更好的提取特征，这就是反向传播的意义。

#### 功能层
卷积神经网络需要哪些额外的输入？
- 非线性激励：卷积是线性运算，增加非线性激励可以增加网络的非线性描述能力；
  - 非线性激励层，一般使用Relu激活函数，它的作用是将特征图中的小于0的值变为0，大于0的值保持不变；
- 降维：特征图稀疏，减少数据运算量，防止过拟合，保持精度；
  - 池化层，pooling layer，也被称为下采样；
  - 一般使用 max pooling、average pooling，目的是进行数据降维，避免过拟合，方便计算、存储；
  - 在池化过程中，每张特征图单独进行降维；
- 归一化：特征的scale保存一致；
  - 归一化层，主要是进行批归一化，Batch Normalization（BN）；
  - 原因：特征的 scale 不一致；
  - 目的：加速训练，提高精度；
  - 算法过程：见图1；
  - 位置：一般放在非线性激励层之前；
- 区域分割：不同区域进行独立学习；
  - 在某些应用中，希望独立对某些区域进行独立学习；
  - 好处：可以学习多套参数，学习得到更强的参数，具备更强的描述能力；
  - 例如：AlexNet
- 区域融合：对分开的区域进行合并，方便信息融合；
  - 对独立进行特征学习的分支进行融合，构建高效而精简的特征组合；
  - 用多种分辨率对目标特征进行多分辨率特征的融合；
  - 例如：Inception model，见图2；
  - 例如：ResNet，见图3；
- 增维：增加图片生成或探测任务中的空间信息；

<div align="center"><img src="../../../../image/批归一化过程.png" width="60%" height="60%"></div>
<div align="center">图1. 批归一化算法过程</div>

<div align="center"><img src="../../../../image/Inception.png" width="60%" height="60%"></div>
<div align="center">图2. Inception Model网络结构</div>

<div align="center"><img src="../../../../image/ResNet.png" width="60%" height="60%"></div>
<div align="center">图3. ResNet.png</div>

### 特性

> 局部连接、参数共享、下采样三大结构特性保障了对图像进行平移、缩放、扭曲后的不变性。  
> 局部连接和参数共享，使得卷积操作能够在输出数据中大致保持输入数据的结构信息。
> 若是将结构化信息输入全连接层，其输出数据会被展成扁平的一维数组，从而丧失输入数据和输出数据在结构上的对应关系。  
> 卷积的局部连接和权值共享等特性，使其具有远小于全连接层的参数量和计算复杂度。

#### 局部连接
> 局部连接，也被称为稀疏交互、局部感受野（Locally Receptive Area）

局部连接体现在：后一层的神经元仅与前一层的部分神经元相连接。这是因为卷积核的尺寸远小于输入特征图的尺寸，而后一层神经元的输入是由卷积核与前一层上的部分神经元做卷积得到的。
局部连接可以使得优化过程的时间复杂度减少几个数量级，同时由于参数量的减少，也使得过拟合的情况得到一定的改善。
具体来讲，假设网络中相临两层分别具有 $m$ 个输入和 $n$ 个输出，那么对于稀疏交互的卷积网络来说，如果限定每个输出与前一层神经元的连接数为 $k, k \ll m$，那么该层的参数总量为 $k \times n$。

局部连接的物理意义是：通常图像、文本、语音等数据都具有局部特征结构，因此我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。

#### 参数共享
> 参数共享，也被称为权值共享（Shared Weights），它指的是在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。

参数共享体现在：一个卷积核可以作用于输入图像多次，只不过每次作用的位置可能不同。

参数共享的物理意义是：使得卷积层具有平移等变性，即无论特征处于特征图中的任何位置，卷积核都可以通过卷积得到它。
也就是说，神经网络的输出对于平移变换来说应当是等变的，即对输入的特征图中，先进行卷积，再进行平移，和对输入先进行平移，再进行卷积得到的输出应当是相等的。

#### 下采样
> 在卷积神经网络中，下采样一般指的是池化操作。

池化的方式有多种，包括最大池化、平均池化、相临重叠区域池化、空间金字塔池化等。

最大池化和平均池化是通过对一定区域内的数值应用 max pooling 或 average pooling 操作得到的。
而相临重叠区域池化则是通过控制步长（stride）小于卷积核尺寸来实现的，即每次滑动卷积核时存在重叠区域。
空间金字塔池化则是通过同时计算多个不同矩阵大小的池化并将结果拼接起来作为下一层的输入实现的，其目的是进行多尺度信息的提取。

### 差异
卷积神经网络和传统神经网络的差异：
- 连接方式
  - 在传统神经网络中，网络层之间的输入和输出的连接关系可以由一个权值参数矩阵来表示，其中每个单独的参数都表示了前后层某两个神经元节点之间的交互。对全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。
  - 而在卷积神经网络中，卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定区域内的神经元存在连接权重（即产生交互），我们称这种特性为稀疏交互。
  - 从特性上来说，卷积神经网络的每层神经元只响应前一层局部区域内的神经元；而全连接网络则每个神经元响应前一层的所有节点。
- 参数共享
  - 在全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；
  - 而在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上；
  - 卷积神经网络的参数共享使得卷积神经网络的参数量和计算量远小于传统的神经网络；

### CNN中的计算
#### 感受野

#### 参数量和计算量的计算
卷积层中的参数量主要取决于每个卷积核的大小以及卷积核的个数：假设卷积核的大小为 $k_w, k_h$，$卷积核的通道数 = 卷积层的输入通道数 = c^{i}$，$卷积核的数量 = 卷积层的输出通道数 = c^{o}$ ，
因此，卷积层的参数量为 $c^{i}  k_w  k_h  c^{o}$。

而卷积层的计算量是由卷积核在每个滑动窗口内的计算量以及整体的滑动次数决定的：卷积核在每个滑动窗口的计算量为每个通道的卷积操作量与通道数的乘积，即 $c^{i} \times k_w \times k_h$；而整体的滑动次数是由输出的特征图的尺度以及通道数
决定的，即 $c^{o} \times l^{o}_{w} \times l^{o}_{h}$；因此，卷积层的计算量为 $c^{i}  k_w  k_h \times c^{o}  l^{o}_{w}  l^{o}_{h}$

#### 输出尺寸的计算
若我们对输入特征图的左右两侧分别进行了 $p_w$ 列填充，上下两侧分别进行了 $p_h$ 行的填充，则填充后的尺寸为 $(l_w^{i} + 2 p_w) \times (l_h^{i} + 2 p_h)$，则输出特征图的尺寸为 

$$
\begin{align}
l_e^{o} = \frac{l_e^{i} + 2 p_e - k_e}{s_e} + 1, \ \ e \in {w, h} \tag{1}
\end{align}
$$

其中，$s_e$ 表示步长。

当步长 $s_e > 1$ 时，会出现非整数的情况，此时会向下取整，即

$$
\begin{align}
l_e^{o} = \lfloor \frac{l_e^{i} + 2 p_e - k_e}{s_e} \rfloor + 1, \ \ e \in {w, h} \tag{2}
\end{align}
$$

在 TensorFlow 中，当指定 `padding = 'same'` 时，会在动在特征图的左右两侧 **一共** 填充 $p_w = k_w - 1$ 列，上下两侧 **一共** 填充 $p_h = k_h - 1$ 行，最终输出的特征图的尺寸为

$$
\begin{align}
l_e^{o} = \lfloor \frac{l_e^{i} - 1}{s_e} \rfloor + 1, \ \ e \in {w, h} \tag{3}
\end{align}
$$

当指定 `padding = 'valid'` 时，不进行填充，而是直接放弃右侧和下侧卷积核无法滑动到的区域，此时输出特征图的尺寸为

$$
\begin{align}
l_e^{o} = \lfloor \frac{l_e^{i} - k_e}{s_e} \rfloor + 1, \ \ e \in {w, h} \tag{4}
\end{align}
$$

### 变种
卷积的变种有：分组卷积（Group Convolution）、转置卷积（Transposed Convolution）、空洞卷积（Dilated / Atrous Convolution）、可变形卷积（Deformable Convolution）。

### 发展
卷积神经网络的整体结构的发展：
- AlexNet：采用了Relu（修正线性单元）作为激活函数（之前使用的是 Sigmoid 函数）、引入了局部响应归一化（Local Response Normalization, LRN）模块、应用了 Dropout 和数据扩充技术来提升训练效果、使用分组卷积来突破当时 GPU 的显存瓶颈。
- VGGNet：卷积核采用 $3 \times 3$ 的尺寸替代之前的 $5 \times 5$、$7 \times 7$ 等大卷积核，这样可以在更少的参数量、更小的计算量下，获得同样的感受野以及更大的网络深度；用 $2 \times 2$ 的池化核替代之前的 $3 \times 3$ 的池化核；去掉了局部响应归一化模块；
- Inception：增加了 Inception 模块（使用多种卷积核来进行多路特征提取）、提出了瓶颈结构、从网络中间拉出多条支线连接辅助分类器，用于计算损失梯度进行误差的反向传播，以缓解梯度消失问题、
将VGGNet网络末端的第一个全连接层换成了全剧平均池化层。
- ResNet：增加了残差块，以解决网络退化的现象（随着网络层数的加深，网络的训练误差和测试误差都会上升，这种现象称为网络的退化）；
- ResNeXt：在 ResNet 中，原残差块

### Text CNN
卷积神经网络的核心思想是捕捉局部特征。
对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于 n-gram。
卷积神经网络的优势在于能够自动地对 n-gram 特征进行组合和筛选，获得不同抽象层次的语义信息。
由于在每次卷积中采用了共享权值的机制，因此它的训练速度相对较快，在实际的文本分类任务中取得了非常不错的效果。

图4是一个用卷积神经网络模型进行文本表示，并最终用于文本分类的网络结构。

<div align="center"><img src="../../../../image/textcnn.png" width="70%" height="70%"></div>
<div align="center">图4. 卷积神经网络在文本分类上的应用</div>

<div align="center"><img src="../../../../image/textcnn网络示意图.png" width="70%" height="70%"></div>
<div align="center">图5. textcnn分类过程示意图</div>

#### 输入层

**「输入层」** 也被称为嵌入层，是一个 $N \times K$ 的矩阵，其中 $N$ 为文章所对应的单词总数，$K$ 是每个词对应的表示向量的维度。
每个词的 $K$ 维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数由网络训练得到。
这两种方法各有优势：预先训练的词嵌入可以利用其他语料库得到更多的先验知识；而由当前网络训练得到的词向量能够更好的抓住与当前任务相关的特征。
因此，图中的输入层实际上采用了两个通道的形式，即有两个 $N \times K$ 的输入矩阵，其中一个是预训练好的词嵌入表达，并且在训练过程中不再发生变化；另一个也是由同样的方式初始化，但是会作为参数，随着网络的训练过程发生改变的。

针对 Embedding Layer 由多种变化：
- CNN-Rand：基础模型，Embedding Layer 中的所有词均被随机初始化，然后模型整体进行训练；
- CNN-Static：模型使用预训练好的 Word2Vec 初始化 Embedding Layer，对于那些在预训练的 Word2Vec 中没有的词，随机初始化；然后固定 Embedding Layer，fine-tune 整个网络；
- CNN-NonStatic：与 CNN-Static 类似，只是 Embedding Layer 不再固定，而是跟随整个网络进行训练；
- CNN-Multichannel：多通道 TextCNN，在 Embedding Layer 有两个 channel，一个是 static 的，一个是 non-static 的，然后整个网络 fine-tune 时只有 non-static 的那个 channel 的参数发生更新，值得注意的是，两个 channel 都是使用预训练的 Word2Vec 初始化。 

#### 卷积层

**「卷积层」** 。由于句子中相邻的单词关联性总是很高，因此可以使用一维卷积来提取特征，即文本卷积和图像卷积的不同之处在于只在文本序列的一个方向（垂直方向）做卷积，卷积核的宽度固定为词向量的维度。

在输入的 $N \times K$ 维矩阵上，我们定义不同大小的滑动窗口进行卷积操作

$$
\begin{align}
c_i = f(w \cdot x_{i:i+h-1} + b) \tag{5}
\end{align}
$$

其中 $x_{i:i+h-1}$ 代表由输入矩阵的第 $i$ 行到第 $i+h-1$ 行所组成的一个大小为 $h \times K$ 的滑动窗口，$w$ 为 $K \times h$ 维的权重矩阵，$b$ 维偏置参数。
假设 $h$ 为 3，则每次在 $3 \times K$ 的滑动窗口上进行卷积，并得到 $N - 2$ 个结果，再将这 $N - 2$ 个结果拼接起来得到 $N - 2$ 维的特征向量。
每一次卷积操作相当于一次特征向量的提取，通过定义不同的滑动窗口，就可以提取出不同的特征向量，得到更丰富的特征表达，构成卷积层的输出。

Note：
- TextCNN 网络包括很多不同窗口大小的卷积核，常用的 filter size 有 ${3, 4, 5}$，每个 filter 的 feature maps $= 100$，这里的特征图就是不同的 $k$ 元语法；
- 如果设置 $padding = 'same'$，即使用宽卷积，则每个 feature maps for each region size 就都是 $seq_len \times 1$，所有的 feature map 可以拼接成 $seq_len \times num_filters$ 的矩阵；
- **通道**：文本中的通道通常是不同方式的 embedding（如 word2vec 或 glove），实践中也有利用静态词向量和 fine-tunning 词向量作为不同 channel 的做法；channel 也可以是一个词序列，而另一个 channel 是对应的词性序列，接下来通过
加或者拼接的方式进行结合；
- feature map 的大小：一般经过尺寸为 `(h, k)` 的卷积核，其生成的特征向量的维度为 `(n-h+1,)`，其中 `n` 表示输入句子的大小；

#### 池化
**「池化层」** 。不同尺寸的卷积核得到的特征（feature map）大小也不相同，因此为了使它们的维度相同，对每个 feature map 使用池化函数。
常用的池化方式有：
- 1-MaxPooling：即从每个滑动窗口产生的特征向量中筛选出一个最大的特征，然后将这些特征拼接起来构成向量表示，这个地方可以使用 Dropout 防止过拟合；
  - 使用 MaxPooling 有几个好处：
    - 首先，这个操作可以保证特征的位置与旋转的不变性，即无论该特征出现在什么位置，都可以将这个强特征提取出来；
    - 其次，MaxPooling 可以减少模型参数的数量，有利于减少过拟合的问题；
    - 最后，对于 nlp 任务来说，MaxPooling 操作可以将变长的输入整理为固定长度的输出；
  - 当然，MaxPooling 也存在一些缺点：
    - 首先，特征的位置信息在这一步中会被完全丢失，事实上，在很多 nlp 任务中，特征出现的位置信息很重要，例如主语出现的位置一般在句子的开头等等；
    - 其次，同一特征的强度信息会丢失，有时有些强特征会出现多次，且出现的次数越多说明这个特征就越重要，但是由于 MaxPooling 只保留一个最大值，这就导致了该强特征的强度信息会被丢失；
- K-MaxPooling：取 feature map 值中的 top-K，并保留这些特征值原始的先后顺序（保序拼接）；
  - 它保留了全局的序列信息，比如在情感分类中，“我觉得这个地方的景色还不错，但是人实在太多了”，这句话的前半部分体现的情感是正向的，但是全局文本表达的情感是偏负面的，因此，利用 k-max pooling 恰好能够捕捉这类的信息；
- AveragePooling：average pooling 就是取 feature map 中的平均值，可以理解为对句子中连续词袋（CBOW）的卷积得到的表示；
- Dynamic Pooling 之 Chunk-Max Pooling，即将某个 filter 对应的 convolution 层的所有特征向量进行分段，切割成若干段后，在每个分段中取最大的特征值，因为是先划分 chunk 再分别取最大值，所以保留了比较粗粒度的位置信息；
  - chunk 的划分有两种方式：一种是静态划分，即事先定义好每个 chunk 的位置；另一种是动态划分；

#### 后续网络层次
在得到文本的向量表示之后，后面的网络结构的设计就与具体的任务相关了。例如，在文本分类场景中，最后可接入一个全连接层，并使用 SoftMax 激活函数输出每个类别的分类。

#### 超参数的选择
- 初始化向量：除了随机初始化 Embedding Layer 之外，一般不直接使用 `one-hot` 来做初始化，而是使用 Word2Vec 或 Glove 初始化（具体依赖于任务本身），且非静态的比静态的效果要好；
- 卷积核的尺寸：**影响较大**，通常滤波器的大小在 $1 \sim 10$ 之间，一般取 $3 \sim 5$，对句子较长的文本（100+），则应该选择大一些的滤波器。
  - 为了找到最优的滤波器的大小，可以使用线性搜索的方法，即在最开始时，我们选用一个 filter，调节 region size 来对比效果，选取最优的 size，然后在这个范围内再调节不同 size 的组合的效果，选取最优的组合；
- 卷积核的数量：**有较大影响**，一般选取 $100 \sim 600$（需要兼顾模型的训练效率），同时使用 Dropout（$0 \sim 0.5$）；卷积核的数量最好不要超过 600，否则容易导致过拟合，可设为 $100 \sim 200$；
- 激活函数：ReLu 和 tanh 两种激活函数的效果表现更佳；
- 池化选择：1-max pooling；
- Dropout 和 正则化：dropout 一般设为 0.5 ，随着 fearure map 数量的增加，性能减少时，可以考虑增大正则化的力度，如尝试大于 0.5 的 dropout；


## 循环神经网络


## 前馈神经网络


## 神经学习中的基础模块
### 批归一化

### DropOut

























