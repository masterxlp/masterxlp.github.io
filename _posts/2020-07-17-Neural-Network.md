---
layout: post
title:  "[D] Neural Network"
date:   2020-07-17 15:00:00 +0800
categories: ML DL NN
tags: 神经网络类总结 D
author: Xlp
---
* content
{:toc}


## 简介

> 本部分包含对各类神经网络中各类问题的总结，包括但不限于背景、原理、特性、差异等。





## 卷积神经网络

> 卷积神经网络（Convolutional Neural Networks, CNN）是一种前馈神经网络，其特点是每层的神经元只响应前一层局部区域内的神经元。

### 背景

> 参考自：
>> [1] [easyai](https://easyai.tech/ai-definition/cnn/)

在 CNN 被提出之前，类似于图像这种的网格结构的数据对于 AI 来说是一个难题，这主要是因为：
- 图像需要处理的数据量太大，导致成本很高，效率很低；
- 图像在数字化的过程中很难保留原有的特征，难以处理图像不变性，导致图像处理的准确率不高；

由于图像是由像素构成的，每个像素又是由颜色（RGB）构成的。
因此，对于一张 $1000 \times 1000$ 像素的图片，需要处理（MLP网络）的参数达到三百万个：$1,000 \times 1,000 \times 3 = 3,000,000$。
这么大的参数量处理起来是非常消耗资源的，而且这还是只是一张不算太大的图片。

卷积神经网络，解决的第一个问题就是：**将复杂问题简化**，即将大量参数降维成少量参数，再做处理。
重要的是：在大部分场景下，降维并不会影结果。
例如，1000像素的图片缩小成200像素并不影响肉眼识别出图片中的是一只猫还是一只狗，机器也是如此

图像数字化的传统方式，简化版如下：

<div align="center"><img src="../../../../image/图像数字化经典方式.png" width="70%" height="70%"></div>

假设有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达（这是由于在普通的前馈神经网络中，只能处理线性的数据，所以需要将网格数据拉平后才能进行进一步的处理）。
但是从视觉的角度来看，图像的内容（本质）并没有发生变化（从空间结构上看，相对位置的信息并没有发生改变），改变的只是位置（像素或者球的位置发生了变化）。
所以，当我们移动图像中的物体（即平移、旋转等）时，用传统的方式得到的参数会有很大差异（因为它破坏了图像的结构信息），这不符合图像处理的要求。

卷积神经网络，解决的第二个问题是：用类似视觉的方式保留图像的特征，即当图像做旋转、平移（变换位置）时，它依然可以有效的识别出类似的图像。

### 原理
#### 卷积层
卷积层时卷积神经网络的基本结构，它是由多个卷积核组合形成的，每个卷积核痛输入数据卷积运算，形成新的特征“图”。
- 卷积核（Convilutional kernel），也被称为滤波器（filter）；
- 卷积核的“矩阵”值，表示为卷积层的参数，其初始值是随机生成的，可通过反向传播更新；
- 卷积核的数量决定了输出的通道数（该层的输出通道数 = 下一层的输入通道数）；
  - 卷积核的数目为超参数，需要用户设定；
  - 一般为 64、128、256；
- 卷积核的形状：[kernel weight, kernel height, input channel]；
- 卷积核的大小是由用户定义的：
  - 奇偶选择：一般奇数，满足中心对称；
  - 大小选择：根据输入数据、图像的特征来决定，一般为 $3 \times 3$、$5 \times 5$、$7 \times 7$；
  - 厚度选择：与输入数据一致，即卷积核的通道数 = 输入通道数；
  - 步长（stride）：对输入特征图的扫描间隔；
  - 边界扩充（padding）：在卷积计算过程中，为了允许边界上的数据也能作为中心参与卷积运算，将边界进行零扩充；
    - 目的：确保卷积后特征图尺度一致；
    - 方法：卷积核的宽度 $2i + 1$，则添加pad宽度为 $i$；
- 每一个卷积核都会与整个输入图像做卷积生成一张特定的特征图，表达了对某一个特征的提取；

卷积层的参数计算：$param\ number = input\ channels \times kernel\ weight \times kernel\ height \times output\ channels (e.g.\ kernel\ number)$。

卷积层的作用：卷积层就是通过卷积核来提取特征的。

反向传播的意义：因为初始的卷积核的值（卷积层的参数）是随机设定的，我们可以根据前向传播的预测结果，进行误差分析，不断的修改卷积核的值，使得卷积核可以更好的提取特征，这就是反向传播的意义。

#### 功能层
卷积神经网络需要哪些额外的输入？
- 非线性激励：卷积是线性运算，增加非线性激励可以增加网络的非线性描述能力；
  - 非线性激励层，一般使用Relu激活函数，它的作用是将特征图中的小于0的值变为0，大于0的值保持不变；
- 降维：特征图稀疏，减少数据运算量，防止过拟合，保持精度；
  - 池化层，pooling layer，也被称为下采样；
  - 一般使用 max pooling、average pooling，目的是进行数据降维，避免过拟合，方便计算、存储；
  - 在池化过程中，每张特征图单独进行降维；
- 归一化：特征的scale保存一致；
  - 归一化层，主要是进行批归一化，Batch Normalization（BN）；
  - 原因：特征的 scale 不一致；
  - 目的：加速训练，提高精度；
  - 算法过程：见图1；
  - 位置：一般放在非线性激励层之前；
- 区域分割：不同区域进行独立学习；
  - 在某些应用中，希望独立对某些区域进行独立学习；
  - 好处：可以学习多套参数，学习得到更强的参数，具备更强的描述能力；
  - 例如：AlexNet
- 区域融合：对分开的区域进行合并，方便信息融合；
  - 对独立进行特征学习的分支进行融合，构建高效而精简的特征组合；
  - 用多种分辨率对目标特征进行多分辨率特征的融合；
  - 例如：Inception model，见图2；
  - 例如：ResNet，见图3；
- 增维：增加图片生成或探测任务中的空间信息；

<div align="center"><img src="../../../../image/批归一化过程.png" width="60%" height="60%"></div>
<div align="center">图1. 批归一化算法过程</div>

<div align="center"><img src="../../../../image/Inception.png" width="60%" height="60%"></div>
<div align="center">图2. Inception Model网络结构</div>

<div align="center"><img src="../../../../image/ResNet.png" width="60%" height="60%"></div>
<div align="center">图3. ResNet.png</div>

### 特性

> 局部连接、参数共享、下采样三大结构特性保障了对图像进行平移、缩放、扭曲后的不变性。  
> 局部连接和参数共享，使得卷积操作能够在输出数据中大致保持输入数据的结构信息。
> 若是将结构化信息输入全连接层，其输出数据会被展成扁平的一维数组，从而丧失输入数据和输出数据在结构上的对应关系。  
> 卷积的局部连接和权值共享等特性，使其具有远小于全连接层的参数量和计算复杂度。

#### 局部连接
> 局部连接，也被称为稀疏交互、局部感受野（Locally Receptive Area）

局部连接体现在：后一层的神经元仅与前一层的部分神经元相连接。这是因为卷积核的尺寸远小于输入特征图的尺寸，而后一层神经元的输入是由卷积核与前一层上的部分神经元做卷积得到的。
局部连接可以使得优化过程的时间复杂度减少几个数量级，同时由于参数量的减少，也使得过拟合的情况得到一定的改善。
具体来讲，假设网络中相临两层分别具有 $m$ 个输入和 $n$ 个输出，那么对于稀疏交互的卷积网络来说，如果限定每个输出与前一层神经元的连接数为 $k, k \ll m$，那么该层的参数总量为 $k \times n$。

局部连接的物理意义是：通常图像、文本、语音等数据都具有局部特征结构，因此我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。

#### 参数共享
> 参数共享，也被称为权值共享（Shared Weights），它指的是在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。

参数共享体现在：一个卷积核可以作用于输入图像多次，只不过每次作用的位置可能不同。

参数共享的物理意义是：使得卷积层具有平移等变性，即无论特征处于特征图中的任何位置，卷积核都可以通过卷积得到它。
也就是说，神经网络的输出对于平移变换来说应当是等变的，即对输入的特征图中，先进行卷积，再进行平移，和对输入先进行平移，再进行卷积得到的输出应当是相等的。

#### 下采样
> 在卷积神经网络中，下采样一般指的是池化操作。

池化的方式有多种，包括最大池化、平均池化、相临重叠区域池化、空间金字塔池化等。

最大池化和平均池化是通过对一定区域内的数值应用 max pooling 或 average pooling 操作得到的。
而相临重叠区域池化则是通过控制步长（stride）小于卷积核尺寸来实现的，即每次滑动卷积核时存在重叠区域。
空间金字塔池化则是通过同时计算多个不同矩阵大小的池化并将结果拼接起来作为下一层的输入实现的，其目的是进行多尺度信息的提取。

### 差异
卷积神经网络和传统神经网络的差异：
- 连接方式
  - 在传统神经网络中，网络层之间的输入和输出的连接关系可以由一个权值参数矩阵来表示，其中每个单独的参数都表示了前后层某两个神经元节点之间的交互。对全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。
  - 而在卷积神经网络中，卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定区域内的神经元存在连接权重（即产生交互），我们称这种特性为稀疏交互。
  - 从特性上来说，卷积神经网络的每层神经元只响应前一层局部区域内的神经元；而全连接网络则每个神经元响应前一层的所有节点。
- 参数共享
  - 在全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；
  - 而在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上；
  - 卷积神经网络的参数共享使得卷积神经网络的参数量和计算量远小于传统的神经网络；

### 变种
卷积的变种有：分组卷积（Group Convolution）、转置卷积（Transposed Convolution）、空洞卷积（Dilated / Atrous Convolution）、可变形卷积（Deformable Convolution）。

### 发展
卷积神经网络的整体结构的发展：
- AlexNet：采用了Relu（修正线性单元）作为激活函数（之前使用的是 Sigmoid 函数）、引入了局部响应归一化（Local Response Normalization, LRN）模块、应用了 Dropout 和数据扩充技术来提升训练效果、使用分组卷积来突破当时 GPU 的显存瓶颈。
- VGGNet：卷积核采用 $3 \times 3$ 的尺寸替代之前的 $5 \times 5$、$7 \times 7$ 等大卷积核，这样可以在更少的参数量、更小的计算量下，获得同样的感受野以及更大的网络深度；用 $2 \times 2$ 的池化核替代之前的 $3 \times 3$ 的池化核；去掉了局部响应归一化模块；
- Inception：增加了 Inception 模块（使用多种卷积核来进行多路特征提取）、提出了瓶颈结构、从网络中间拉出多条支线连接辅助分类器，用于计算损失梯度进行误差的反向传播，以缓解梯度消失问题、
将VGGNet网络末端的第一个全连接层换成了全剧平均池化层。
- ResNet：增加了残差块，以解决网络退化的现象（随着网络层数的加深，网络的训练误差和测试误差都会上升，这种现象称为网络的退化）；
- ResNeXt：在 ResNet 中，原残差块

### Text CNN


## 循环神经网络


## 前馈神经网络