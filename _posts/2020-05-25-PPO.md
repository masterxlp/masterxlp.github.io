---
layout: post
title:  "[B] Proximal Policy Optimization Algorithms"
date:   2020-05-25 10:24:00 +0800
categories: RL
tags: PPO B
---
* content
{:toc}

## [B] Proximal Policy Optimization Algorithms
### 脉络分析
PPO论文实际上时提出了一个 `clipped surrogate objective`，并通过实验证明该目标函数表现确实不错（实际上还是分场景的）。

从 **Policy Gradient** 开始，其目标函数为 $$L^{PG}(\theta) = \hat{\mathbb{E}}_t [log\pi_{\theta}(a_t|s_t)\hat{A}_t]$$。
直观地，其策略更新依赖于 **策略** $\pi(\theta)$ 以及 **优势值**  $\hat{A}_t$。
事实上，策略梯度方法的更新方式为：依据于当前的策略，通过与环境交互收集一批数据，基于这批数据执行一次策略更新。
因此，它属于 **on-policy** 的算法。
对于on-policy的方法，如果利用“旧”数据（$$\pi_{old}$$ 收集到的样本）进行策略更新会导致 **过大的策略更新**。
那么，为什么要使用 **off-policy** 的方法呢？
这是因为 **data efficiency** 的缘故，on-policy方法对于数据的利用效率太低。




而 **Trust-Region Policy Optimization** 通过数学推导提出了一种 **surrogate objective**，这种目标函数在对策略更新幅度上加以限制后，可以做到策略的单调提升。
其目标函数为 

$$
\begin{align}
\mathop{maximize}\limits_{\theta} \quad \hat{\mathbb{E}}_t [\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A}_t]
\end{align}
$$

$$
\begin{align}
subject\ to\ \hat{\mathbb{E}}_t [KL[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]] \leq \delta
\end{align}
$$

这是一个带约束的目标函数，根据拉格朗日乘子法可将带约束的函数转换为带拉格朗日乘子（惩罚）的目标函数：

$$
\begin{align}
\mathop{maximize}\limits_{\theta} \quad \hat{\mathbb{E}}_t [\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t - \beta KL[\pi_{\theta_{old}}(\cdot|s_t), \pi_{\theta}(\cdot|s_t)]]
\end{align}
$$

但是在现实中，我们无法确定一个固定的 $\beta$ 来适应于不同的任务，甚至是同一种任务的不同状态。
但是，TRPO又拥有不可多得的优势：可以guarantee策略的稳步上升，或者至少不会下降。

基于此，提出了PPO方法，该方法通过对 **probability ratio** 加以截断，即限制 $r_t(\theta)$ 的大小，当 $r_t(\theta)$ 使得目标函数减少时，截断它，使得目标函数不会减少。
通过截断的方式达到与TRPO相同的效果，但是却比TRPO要更简单，因为它是优化一个无约束的目标函数。
其目标函数为：

$$
\begin{align}
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t)]
\end{align}
$$

文章中通过实验证明，当 $\epsilon = 0.2$ 时，模型性能最佳。

### 模型流程图
基于DeepMind的baselines，画出了其PPO2算法的流程图，如下：  
<div align=center> ![Figure 1](../../../../image/ppo网络结构图.png "ppo structure") </div>

另一种attention结构+mask机制的PPO算法流程图，如下：
![Figure 2](../../../../image/alpha-star-ppo网络结构图.png "attention structure")
