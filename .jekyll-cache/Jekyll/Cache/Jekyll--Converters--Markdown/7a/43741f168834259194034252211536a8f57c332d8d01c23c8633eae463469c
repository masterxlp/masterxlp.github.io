I"U_<ul id="markdown-toc">
  <li><a href="#算法实现" id="markdown-toc-算法实现">算法实现</a></li>
  <li><a href="#lstm" id="markdown-toc-lstm">lstm</a>    <ul>
      <li><a href="#原理" id="markdown-toc-原理">原理</a></li>
      <li><a href="#tensorflow实现" id="markdown-toc-tensorflow实现">Tensorflow实现</a></li>
    </ul>
  </li>
  <li><a href="#api" id="markdown-toc-api">api</a>    <ul>
      <li><a href="#get_variabel" id="markdown-toc-get_variabel">get_variabel</a></li>
      <li><a href="#initializer-合集" id="markdown-toc-initializer-合集">initializer 合集</a></li>
    </ul>
  </li>
  <li><a href="#其他" id="markdown-toc-其他">其他</a>    <ul>
      <li><a href="#slim" id="markdown-toc-slim">slim</a>        <ul>
          <li><a href="#示例" id="markdown-toc-示例">示例</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>该部分记录了本人对Tensorflow的api以及某些算法的Tensorflow实现</p>
</blockquote>

<h2 id="算法实现">算法实现</h2>
<h2 id="lstm">lstm</h2>
<h3 id="原理">原理</h3>
<p>LSTM可处理序列式的数据，其每个cell是由三种门限机制组成的，详见图1.</p>

<div align="center"><img src="../../../../image/LSTM结构图.png" width="60%" height="60%" /></div>
<div align="center">图1. LSTM cell结构图</div>

<div align="center"><img src="../../../../image/LSTM中的符号说明.png" width="60%" height="60%" /></div>
<div align="center">图2. LSTM cell结构图中的符号说明</div>

<p>可以看出，遗忘门实际上做的是一个sigmoid的计算，其输入为上一个cell的隐藏状态 $h_{t-1}$ 以及当前时刻的输入 $X_t$；
记忆门做了两个事，一个是sigmoid计算，一个tanh计算，其中sigmoid的计算的输入与tanh的计算的输入一样，为 上一个cell的隐藏状态 $h_{t-1}$ 以及当前时刻的输入 $X_t$；
输出门也是做了一个sigmoid的计算，其输入也为上一个cell的隐藏状态 $h_{t-1}$ 以及当前时刻的输入 $X_t$；
然后，新状态的值为遗忘门的输出与上一个状态的值的点积加上记忆门的两个输出结果的点积；
最后，隐藏状态的值为输出门的输出与新状态的tanh计算后的输出的点积。</p>

<h3 id="tensorflow实现">Tensorflow实现</h3>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 生成各个门的参数</span>
<span class="k">def</span> <span class="nf">_generate_params_for_lstm_cell</span><span class="p">(</span><span class="n">x_size</span><span class="p">,</span> <span class="n">h_size</span><span class="p">,</span> <span class="n">bias_size</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="s1">'x_weights'</span><span class="p">,</span> <span class="n">x_size</span><span class="p">)</span>
    <span class="n">h_w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="s1">'h_weights'</span><span class="p">,</span> <span class="n">h_size</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="s1">'biases'</span><span class="p">,</span> <span class="n">bias_size</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x_w</span><span class="p">,</span> <span class="n">h_w</span><span class="p">,</span> <span class="n">b</span>

<span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">num_embedding_size</span> <span class="o">+</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mf">3.0</span>
<span class="n">lstm_init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

<span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="s1">'lstm_nn'</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">lstm_init</span><span class="p">):</span>
    <span class="c1"># 生成参数</span>
    <span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="s1">'inputs'</span><span class="p">):</span>
        <span class="n">ix</span><span class="p">,</span> <span class="n">ih</span><span class="p">,</span> <span class="n">ib</span> <span class="o">=</span> <span class="n">_generate_params_for_lstm_cell</span><span class="p">(</span>
            <span class="n">x_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_embedding_size</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">h_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">bias_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="p">)</span>

    <span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="s1">'outputs'</span><span class="p">):</span>
        <span class="n">ox</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ob</span> <span class="o">=</span> <span class="n">_generate_params_for_lstm_cell</span><span class="p">(</span>
            <span class="n">x_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_embedding_size</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">h_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">bias_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="p">)</span>

    <span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="s1">'forget'</span><span class="p">):</span>
        <span class="n">fx</span><span class="p">,</span> <span class="n">fh</span><span class="p">,</span> <span class="n">fb</span> <span class="o">=</span> <span class="n">_generate_params_for_lstm_cell</span><span class="p">(</span>
            <span class="n">x_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_embedding_size</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">h_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">bias_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="p">)</span>

    <span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="s1">'memory'</span><span class="p">):</span>
        <span class="n">cx</span><span class="p">,</span> <span class="n">ch</span><span class="p">,</span> <span class="n">cb</span> <span class="o">=</span> <span class="n">_generate_params_for_lstm_cell</span><span class="p">(</span>
            <span class="n">x_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_embedding_size</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">h_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">bias_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="p">)</span>

    <span class="c1"># 定义状态和隐藏状态</span>
    <span class="c1"># state 表示上一个cell的细胞状态</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="no">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="n">trainable</span><span class="o">=</span><span class="no">False</span><span class="p">)</span>
    <span class="c1"># h 表示上一个cell的隐藏状态</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="no">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_lstm_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="n">trainable</span><span class="o">=</span><span class="no">False</span><span class="p">)</span>

    <span class="c1"># 根据每一时刻的输入执行cell的计算</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">):</span>
        <span class="c1"># embed_input 表示第t时刻的输入</span>
        <span class="n">embed_input</span> <span class="o">=</span> <span class="n">embed_input</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>     <span class="c1"># 第一个是取batch_size（全取），第二个是取第i个词，第三个维度表示embedding的大小</span>
        <span class="n">embed_input</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_embedding_size</span><span class="p">])</span>

        <span class="c1"># 计算遗忘门的输出</span>
        <span class="n">forget_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span> <span class="n">fx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">fh</span><span class="p">)</span> <span class="o">+</span> <span class="n">fb</span><span class="p">)</span>    <span class="c1"># sigmoid(WX + Wh + b)</span>
        <span class="c1"># 计算输入门的输出</span>
        <span class="n">input_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">ih</span><span class="p">)</span> <span class="o">+</span> <span class="n">ib</span><span class="p">)</span>
        <span class="c1"># 计算输出门的输出</span>
        <span class="n">output_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span> <span class="n">ox</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">oh</span><span class="p">)</span> <span class="o">+</span> <span class="n">ob</span><span class="p">)</span>
        <span class="c1"># 中间状态的计算</span>
        <span class="n">mid_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span> <span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">ch</span><span class="p">)</span> <span class="o">+</span> <span class="n">cb</span><span class="p">)</span>
        <span class="c1"># 计算新的细胞状态</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">mid_state</span> <span class="o">*</span> <span class="n">input_gate</span> <span class="o">+</span> <span class="n">state</span> <span class="o">*</span> <span class="n">forget_gate</span>         <span class="c1"># mid_state * input_gate: 输入门与中间态的点积为记忆门的输出</span>
        <span class="c1"># 计算新的隐藏状态</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">output_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="n">last</span> <span class="o">=</span> <span class="n">h</span>   <span class="c1"># 最后一个cell的输出</span>
</code></pre></div></div>

<h2 id="api">api</h2>
<h3 id="get_variabel">get_variabel</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.get_variable(
    name,                       # 变量的名称
    shape = None,               # 变量的形状
    dtype = None,               # 变量的数据类型，默认为 DT_FLOAT
    initializer = None,         # 变量的初始化方式，默认为None，表示使用变量域内已经定义好的初始化方法，如果变量域内没有定义，那么使用 glorot_uniform_initializer 进行初始化
    regularizer = None,         # 一个标准化函数，默认为None，表示使用变量域内已经定义好的标准化函数，如果不存在，那么不进行标准化
    trainable = True,           # 是否加入计算图进行训练，默认为True
    collections = None,         # 将变量加入到图的collections列表中，默认添加到GraphKeys.GLOBAL_VARIABLES中
    caching_device = None,      # 设备字符串或者函数，表明读取变量缓存的位置，默认为变量的设备
    partitioner = None,         # 接收一个完全定义了TensorShape和dtype的变量，并返回一个为每个轴分区的列表
    validate_shape = True,      # 如果为False，那么允许变量被初始化为一个未知shape的值，否则，必须用一个已知shape的值来初始化变量
    use_resource = None,        # 如果为False，创建一个regular的变量，否则，创建一个具有良好语义定义的experimental ResourceVariable，默认为False
    custom_getter = None,
    constraint = None,
)

Returns: 
    返回一个新创建的或已经存在的Tensor变量
</code></pre></div></div>

<h3 id="initializer-合集">initializer 合集</h3>
<blockquote>
  <p>该小结是对tensorflow中的各种初始化方法的总结</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 常量初始化函数
tf.constant_initializer()

# 满足正态分布的初始化
tf.random_normal_initializer()

# 满足截取的正太分布的初始化
tf.truncated_normal_initializer()

# 满足均匀分布的初始化
tf.random_uniform_initializer()

# 满足均匀分布，但不影响输出数量级的随机值初始化
tf.uniform_unit_scaling_initializer()

# 零初始化
tf.zeros_initializer()

# 一初始化
tf.ones_initializer()
</code></pre></div></div>

<h2 id="其他">其他</h2>
<h3 id="slim">slim</h3>
<blockquote>
  <p>slim 是TensorFlow的简洁版。</p>
</blockquote>

<h4 id="示例">示例</h4>
<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">tensorflow</span> <span class="n">as</span> <span class="n">tf</span>
<span class="n">import</span> <span class="n">tensorflow</span><span class="p">.</span><span class="nf">contrib</span><span class="p">.</span><span class="nf">slim</span> <span class="n">as</span> <span class="n">slim</span>

<span class="c1"># 原生卷积层定义</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="s1">'conv1'</span><span class="p">)</span> <span class="n">as</span> <span class="ss">scope:
    </span><span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="n">scope</span><span class="p">.</span><span class="nf">name</span> <span class="o">+</span> <span class="s1">'w_1'</span><span class="p">,</span>
                              <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
                              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">float32</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">float32</span><span class="p">))</span>
    <span class="n">biases</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="n">scope</span><span class="p">.</span><span class="nf">name</span> <span class="o">+</span> <span class="s1">'b_1'</span><span class="p">,</span>
                             <span class="p">[</span><span class="mi">16</span><span class="p">],</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">float32</span><span class="p">,</span>
                             <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">constant_initializer</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">nn</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'SAME'</span><span class="p">)</span>
    <span class="n">pre_activation</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">nn</span><span class="p">.</span><span class="nf">bias_add</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
    <span class="n">conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">pre_activation</span><span class="p">,</span> <span class="nb">name</span><span class="o">=</span><span class="n">scope</span><span class="p">.</span><span class="nf">name</span><span class="p">)</span>

<span class="c1"># slim 版本</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">slim</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">scope</span><span class="o">=</span><span class="s1">'conv1'</span><span class="p">)</span>
<span class="c1"># inputs 就是网络输入，16是输出神经元的个数，[3,3]是该层卷积核的大小</span>
</code></pre></div></div>

:ET