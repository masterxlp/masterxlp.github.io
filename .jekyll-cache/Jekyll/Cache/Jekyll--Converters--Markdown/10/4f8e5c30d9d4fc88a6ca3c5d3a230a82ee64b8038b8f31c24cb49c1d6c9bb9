I"3<ul id="markdown-toc">
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
  <li><a href="#说明" id="markdown-toc-说明">说明</a></li>
  <li><a href="#impala-过程" id="markdown-toc-impala-过程">IMPALA 过程</a>    <ul>
      <li><a href="#single-learner" id="markdown-toc-single-learner">Single Learner</a></li>
      <li><a href="#multiple-synchronous-learners" id="markdown-toc-multiple-synchronous-learners">Multiple Synchronous Learners</a></li>
    </ul>
  </li>
  <li><a href="#v-trace" id="markdown-toc-v-trace">V-Trace</a></li>
  <li><a href="#actor-critic" id="markdown-toc-actor-critic">Actor-Critic</a></li>
</ul>

<p>转载自<a href="https://zhuanlan.zhihu.com/p/58226117">知乎</a></p>

<h2 id="abstract">Abstract</h2>
<blockquote>
  <p>这篇文章提出了一个可用于大规模强化学习训练的框架 – IMPALA，其主要思想是通过大量的与训练无关的Actor来采集样本，将训练集中于Learner中，这样采样与训练便可以并行运行，
该框架具有较高的性能、较好的扩展性以及较高的数据效率。</p>
</blockquote>

<blockquote>
  <p>V-Trace 技术则是用来矫正由于采样与训练并行时导致的策略错位情况，通过V-Trace技术完成使用Off-Policy的样本进行训练的目的。</p>
</blockquote>

<h2 id="说明">说明</h2>
<p>在文章中，各个符号具有以下含义：</p>
<ul>
  <li>$x_t$ ：状态</li>
  <li>$a_t$ ：动作</li>
  <li>$r_t$ ：奖励</li>
  <li>$\mu(a_t \mid x_t)$ ：策略分布</li>
</ul>

<h2 id="impala-过程">IMPALA 过程</h2>

<div align="center"><img src="../../../../image/impala.png" width="60%" height="60%" /></div>

<h3 id="single-learner">Single Learner</h3>
<p>在single learner模式中，learner的主要作用是通过获取actor采样的轨迹进行SGD以更新各个神经网络的参数，在文章中，learner单独使用一块GPU进行训练。
而actor则是定期的从learner中获取最新的神经网络参数（最新的策略），并以此参数在自己独立的模拟环境（一个actor对于一个独立的模拟环境）中进行采样，
并把自己采集到的轨迹 $\lbrace x_t, a_t, r_t, \mu(a_t|x_t) \rbrace$ 通过 queue 传递给learner以供learner进行下一轮的参数更新。
由于模拟环境的运行通常不便于做并行，因此actor一般使用CPU。</p>

<h3 id="multiple-synchronous-learners">Multiple Synchronous Learners</h3>
<p>当训练规模进一步扩大时，考虑使用multiple synchronous learners模式，它使用多个learner（每个learner使用一块GPU），每个learner有配合多个actor（CPU）。
每个learner只从它们自己的actor群中获取样本进行参数的更新。
但是，learner之间可以定期交换梯度用于进行网络更新。
与single learner一样，actor会定期的从任意learner上获取最新的网络参数（策略）。</p>

<p>IMPALA中的actor和learner相互异步工作，极大提高了时间利用率</p>

<p>文章与Batched A2C做了对比，如图2所示。</p>

<div align="center"><img src="../../../../image/batcha2c-impala.png" width="60%" height="60%" /></div>

<p>图2(a)中，正向传播和反向传播都凑成一批来做，因此每一步都需要同步，而模拟环境各步所需时间的方差却很大，这样就导致了浪费大量的时间在做等待；
图2(b)中，只把耗时较长的反向传播凑层一批来做，而正向传播就给各个actor自己做，这样还是有一部分的时间浪费在了等待上；
图2(c)中，IMPALA则完全把actor和learner分开，异步进行，这样actor不用去等待别的actor，可以尽量多的做采样，这是以牺牲每次得到的on-policy样本实现的。
为了对off-policy的样本做矫正，使得off-policy的样本可以用于learner的训练，作者提出了V-Trance技术。</p>

<h2 id="v-trace">V-Trace</h2>
<p>算法需要根据采样得到的样本来维护一个状态价值函数 $V_\theta (x)$。
V-Trace的目的是根据采样到的 $\lbrace x_t, a_t, r_t, \mu(a_t \mid x_t) \rbrace$ 和当前状态价值函数网络来给出当前状态价值函数的一个更好的估计 $v_s$（$s$ 下标表示他是其中的一个样本），
这样价值神经网络就可以把它作为一个更新的目标来更新权重。</p>

<p>$v_s$ 的表达式被定义为：</p>

\[\begin{align}
&amp;v_s \doteq V(x_s) + \sum_{t=s}^{s+n-1} \gamma^{t-s} (\prod_{i=s}^{t-1} c_i) \delta_t V \\
&amp;\delta_t V \doteq \rho_t (r_t + \gamma V(x_{t+1} - V(x_t))) \\
&amp;\rho_t \doteq min(\bar{\rho}, \frac{\pi(a_t \mid x_t)}{\mu(a_t \mid x_t)}) \\
&amp;c_i \doteq min(\bar{c}, \frac{\pi(a_i \mid x_i)}{\mu(a_i \mid x_i)})\ such\ that\ \bar{\rho} &gt;= \bar{c}
\end{align}\]

<p>其中，$\delta_t V$ 是 $V$ 的一个时间差分（temporal difference），$\rho_t$ 和 $c_i$ 是截断的重要性采样权重（truncated importance sampling weights）。
另外，当 $s = t$ 时，我们令 $\prod_{i=s}^{t-1} c_i = 1$；且截断满足 $\bar{\rho} &gt;= \bar{c}$。</p>

<p>它具有如下性质：</p>
<ul>
  <li>状态价值函数 $V_\theta(x)$ 每次向 $v_s$ 上更新，最后都能够收敛</li>
  <li>状态价值函数 $V_\theta(x)$ 每次向 $v_s$ 上更新，收敛到的状态价值函数时介于 $V^\pi$ 和 $V^\mu$ 之间的某个价值函数，我们记该价值函数为 $V^{\pi_{\bar{\rho}}}$，该价值函数对于的策略表示为</li>
</ul>

\[\begin{align}
\pi_{\bar{\rho}}(a \mid x) \doteq \frac{min(\bar{\rho} \mu(a \mid x), \pi(a \mid x))}{\sum_{b \in A} min(\bar{\rho} \mu(b \mid x), \pi(b \mid x))}
\end{align}\]

<ul>
  <li>为了避免 importance weight 发散，需要加上相应的上界</li>
  <li>参数 $\bar{\rho}$ 决定了收敛到的不动点的位置</li>
  <li>$\bar{c}$ 和 $\bar{\rho}$ 决定了收敛的速率</li>
  <li>在 on-policy 的情况下，如果 $\bar{\rho} &gt;= \bar{c} &gt;= 1$ 的话，那么 $v_s$ 就退化为了 on-policy n-steps Bellman target</li>
</ul>

<h2 id="actor-critic">Actor-Critic</h2>
<p>在IMPALA中需要维护两个神经网络，一个是策略神经网络（用作actor），一个是状态价值函数网络（用作critic）。</p>

:ET