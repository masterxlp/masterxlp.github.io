I"'<h2 id="c-多目标强化学习总结">[C] 多目标强化学习总结</h2>
<h3 id="wuji-automatic-online-combat-game-testing-using-evolutionary-deep-reinforcement-learning">Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning</h3>
<blockquote>
  <p>2019年 - 34th IEEE/ACM International Conference on Automated Software Engineering - 软件工程领域顶级会议论文<br />
Author: 天津大学强化学习实验室、网易伏羲人工智能实验室、NTU<br />
Link: <a href="https://yanzzzzz.github.io/files/PID6139619.pdf">原文链接</a><br />
参考自: <a href="https://www.jiqizhixin.com/articles/2019-10-22-11">机器之心</a></p>
</blockquote>

<h4 id="背景">背景</h4>
<p>如今在开发的各种游戏中存在各种各样的游戏缺陷（BUG），为了发现这些BUG游戏测试应运而生。在工业界，如今的游戏测试一般都还表现脚本测试以及手工测试相结合的形式。
时至今日，自动化游戏测试的研究仍然处于初级阶段，一个主要原因是玩游戏本身是一个持续决策的过程，而BUG往往隐藏的较深，只有当某些困难的中间任务完成后，
才有可能被触发，这就要求游戏测试算法拥有类人的智能。
近年来，深度强化学习算法（DRL）取得的非凡的成功，特别在游戏控制领域，甚至表现出了超越人类的智能，这为推进自动化游戏测试提供了启示。
然而，既有的 DRL 算法主要关注如何赢得游戏，而不是游戏测试，导致其可能无法广泛地覆盖需要测试的分支场景。
<img src="../image/Wuji中智能体与环境的交互示意图.png" alt="Figure 1" title="Wuji中智能体与环境的交互示意图" /></p>

<h4 id="wuji优势概括">wuji优势概括</h4>
<p>Wuji融合了进化算法、深度强化学习算法以及多目标优化机制，实现了智能的自动化游戏测试，在赢得游戏和探索游戏空间（两个目标）之间取得了较好的平衡，
其中，赢得游戏可以使得智能体在游戏中取得进展，而空间探索则可以增加发现BUG的可能性。</p>

<p><img src="../image/Wuji结构图.png" alt="Figure 2" title="Wuji结构图" /></p>

<h4 id="wuji---基于多目标优化的进化强化学习框架">Wuji - 基于多目标优化的进化强化学习框架</h4>
<p>从强化学习算法的角度看，不同的策略 $\pi$ 都能够探索到游戏中不同的状态空间。
从进化算法的角度看，通过维护一个策略“种群”，可以实现游戏空间的搞笑探索。
直观上，可以将二者结合，实现有效的游戏测试。
Wuji正式构建在这样的强化学习架构之上的（见图3）。</p>

<p>然而，进化算法需要选择优质的后代，如前文所述，使用胜率作为策略的单一衡量指标会使得种群内的策略都趋同于取胜，无法探测到更广泛的游戏空间，降低游戏测试效果。
为此，Wuji使用 <code class="highlighter-rouge">多目标优化机制</code>，<strong>对每个策略分别从胜率以及空间探索能力两个维度衡量策略性能</strong>，并以此进行优质后代的选择。</p>

<p>具体来说，每个策略用于后代选择的 <em>Fitness-value (FV)</em> 计算方式如下：</p>

\[\begin{align}
FV(\pi) = [RS_G^{\pi}, ES_G^{\pi}]
\end{align}\]

<p>例如，给定游戏 $G$，使用策略 $\pi$ 执行一个回合后，$RS_G^{\pi}$ 表示策略在当前回合的胜率，$ES_G^{\pi}$ 表示策略在当前回合中探索状态空间的数量。
至此，策略的 $FV$ 从标量扩展到了向量。
因此，后代选择的方式也从选择较大的标量转变为了 <strong>向量之间的比较</strong>。</p>

<p>在文章中，作者提出使用非支配排序（non-dominate sorting，NDS）来选择非支配集（non-dominate set），进而选择更优质的后代。
具体过程见图3(右)所示，图中每一个点表示一个策略，两个维度衡量了策略在获胜能力和探索能力两个维度上的表现。
在整个种群中存在一个集合 $F_1$，该集合中的策略相互不支配（例如 $\pi_1$ 的胜率比 $\pi_2$ 高，但是探索能力相比来说要低，因此，无法说明这两个策略谁更优秀），
该策略集又被称为帕累托前沿(Pareto Frontier)。</p>

<p>基于此，进行后代选择的时候，优先选择集合中的帕累托前沿 $F_1$，接着从种群中剔除 $F_1$ 后再进行非支配集的筛选，找到第二个帕累托前沿 $F_2$，再加入到后代中，
循环往复直到种群数量达到上限。
值得注意的是，当将 $F_3$ 加入到后代种群中时，如果遇到种群规模超出上限阈值的情况，需要针对 $F_3$ 内的策略进行筛选。
具体见算法(2-4)</p>

<p><img src="../image/Wuji算法1.png" alt="Figure 3" title="Wuji算法1" /></p>

<p>为此，提出使用 <strong>聚集距离（crowding distance）对策略的密集程度进行度量</strong>，
并基于聚集距离实现策略的聚集距离排序算法（Crowding distance sorting，CDS）实现策略的末位淘汰。
例如，如图3(b)所示，针对策略 $\pi_1$ 的聚集距离定义如下：</p>

\[\begin{align}
CD(\pi_1) = d_1 + d_2 + d_3 + d_4
\end{align}\]

<p>其中 $d_1$ 和 $d_4$ 衡量了在探索能力的维度上，距离 $\pi_1$ 最近的邻策略的距离，$d_2$ 和 $d_3$ 衡量了胜率维度的距离。
根据聚集距离对策略进行CDS，保留聚集距离较大的策略，淘汰聚集距离较小的策略，以此实现策略的多样性。
CDS尽可能选择两端的策略，以及均匀分布在两个极端之间的策略，以实现后代策略的多样性。</p>

<p><img src="../image/Wuji算法2.png" alt="Figure 4" title="Wuji算法2" /></p>

<p>综上所述，Wuji 借助进化强化学习算法框架，结合多目标优化机制，使得种群内的策略朝着胜率以及探索能力两个方向不断优化，
同时还保证部分策略均匀的分布在两个优化目标之间。二者的融合使 Wuji 能够完成更多任务并探索游戏的更多状态，提升发现 bug 的几率。</p>

<h3 id="direct-feature-prediction">Direct Feature Prediction</h3>
<blockquote>
  <p>Author1: Arthur Juliani<br />
Author2: 王瀚宸<br />
Link1: <a href="https://arxiv.org/abs/1611.01779">原文链接</a><br />
参考自<a href="https://mp.weixin.qq.com/s/XHdaoOWBgOWX7SrOemY4jw">量子位</a></p>
</blockquote>

<p>强化学习的主要内容就是不断的训练agent完成任务，我们认为这会让agent学会做这件事。
举例来说，假如我们希望训练一个会开门的机器人，或者叫agent，以强化学习为框架，就可以让机器人在不断试错中学会开门。</p>

<p>但如果我们希望agent能够完成多个目标呢，比如完成的目标需要随着时间变化而变化，那这时需要怎样做呢？</p>

<h4 id="q-learning--为了最大的累积奖励">Q-learning : 为了最大的累积奖励</h4>
<p>强化学习就是让agent在某个环境中不断的依据某种策略进行互动，其目标就是让agent随着时间的推移获得最大累积奖励。
这个过程往往以如下形式进行：一个agent从环境中接收到一个状态 $s \in \mathcal{S}$，进而依据策略 $\pi$ 产生相应的动作 $a \in \mathcal{A}$，
执行该动作，agent接收一个即时奖励 $r \in \mathbb{R}$ 以及下一个状态 $s’ \in \mathcal{S}$。
强化学习解决的问题就是学习一个从状态到动作的映射，即策略 $\pi$，以保证能够产生最大累积奖励。</p>

<p>Q-learning就是用来解决这类问题的一种方法：它学习的是状态-动作对与价值的估计值 $V$ 之间的直接关系$。
该估计值与在状态 $s$ 下采取动作 $a$ 所得到的期望累积折扣奖励相对应。
集合贝尔曼方程，我们可以通过迭代得到所有可能的“状态-动作对”的 $Q$ 估计值。
这种Q值迭代的方法来自于优化Q方程的一下性质：</p>

\[\begin{align}
Q^{*}(s,a) = r + \gamma \mathop{max}\limits_{a'}Q(s',a') \tag{1}
\end{align}\]

<p>上式的含义为：对于给定的状态 $s$ 以及动作 $a$，其当前的Q值可以分解为当前获得的即时奖励 $r$ 与下一状态的期望累积折扣奖励的和。
通过收集experience，我们训练的神经网络能够随着时间的推移越来越准确的估计真实Q值。，随后通过采取最优值的动作，理论上既能够从环境中获得最大的累积奖励。</p>

<p>使用一个诸如神经网络的全局功能近似器，我们能够对未发生的状态归纳出其最接近真实Q值的Q的估计值，从而使我们能够了解任意大的状态空间下的Q方程。</p>

<h4 id="无人机送货--基于目标的强化学习">无人机送货 &amp; 基于目标的强化学习</h4>
<p>Q-learning和其他传统的强化学习算法都采用单一奖励信号，因此也只能完成单一目标。</p>

<p>在我们设置的环境中，agent将占据 5 * 5 方格拦中的一个位置，然后目的地在另外一个位置，这个agent可以在上下左右四个方向上随意移动。
如果我们希望无人机能够学会运送货物，我们通常会在无人机成功飞到标记点、完成送货时，提供一个+1的正向奖励。</p>

<p>&lt;div align=”center”, width=200px, height=200px&gt;<img src="../image/无人机送货示意图.png" />&lt;/div&gt;</p>

<p>上图展示了我们agent的学习内容和环境，我们将会使用 5 * 5 的RGB网格(共有75种可能)这样一个更简单的形式表示环境。
这会把学习的时间从小时量级降低到分钟量级。
每个training episode中agent可以移动100步，在每个episode开始之前会随机分配agent和目的地的位置。</p>

<h4 id="使用tensorflow实践q-learning">使用Tensorflow实践Q-learning</h4>
<p>一下所示的使用Tensorflow执行Q-learning算法的异步版本，即通过同时运行多个agent来学习策略，这能在加速训练过程中同时增加稳定性。
具体实现过程见：<a href="https://github.com/awjuliani/dfp/blob/master/Async-Q.ipynb">链接</a></p>

<p>我们在一台机器上</p>

:ET