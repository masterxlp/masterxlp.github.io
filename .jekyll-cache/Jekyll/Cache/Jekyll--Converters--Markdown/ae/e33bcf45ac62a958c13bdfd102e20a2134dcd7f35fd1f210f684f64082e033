I"<h2 id="a-unicorn-continual-learning-with-a-universal-off-policy-agent">[A] Unicorn: Continual learning with a universal, off-policy agent</h2>
<h3 id="abstract">Abstract</h3>
<p>一些real-word领域最好的描述是作为一个单任务来描述，但是对于其他领域来说，这种观点是有局限性的。
相反，随着智能体的competence的提高，一些任务的复杂性也不断增加。
在持续学习中（也称为终身学习），没有明确的任务边界或curricula。
随着学习中的智能体变的越来越强大，持续学习仍然是阻碍快速进步的前沿领域之一。
为了测试持续学习的能力，我们考虑一个具有挑战性的3D领域，该领域具有隐式的任务序列和稀疏的奖励。
我们提出了一种新颖的智能体结构，“独角兽(Unicorn)”，它展示了强大的持续学习能力，并在上面提出的领域中超过了几种基线智能体的表现。
智能体通过联合表示以及使用一个并行的off-policy的设定高效的学习多个策略来实现这一点。</p>

<h3 id="introduction">Introduction</h3>
<p>持续学习，是一种利用之前获得的知识或技巧的方式从有关连续任务的经验中学习的方法，它一直是人工智能领域的一个长期挑战。
该方法的一个主要优点就是，它为一个完全自主的智能体提高了增量地构构建其能力以及在不需要人为的提供数据集、任务边界或reward shaping的情况下解决在丰富(rich)、复杂的环境中表示它的挑战的潜力。
相反，随着智能体能力的提升，它会考虑增加任务的复杂性。
一个理想的持续学习智能体应该能够：(A) 解决多种任务；(B) 当任务相关时表现出协同效应； (C) 处理任务之间的深度依赖结构（例如，一把锁只有当它的钥匙被拿起时才能解锁）。</p>

<p>在监督学习设置下存在着大量连续学习技术。
然而，正如 Parisi et al. 提到的那样，需要更多的研究来解决在不确定的环境中使用自主的智能体进行持续学习的问题 – 这非常适合强化学习。
以前使用关于强化学习的持续学习工作，特别是关于使用深度依赖结构解决任务的工作，通常focused on将学习分为两个阶段：首先，要求分别获得各自的skills，然后再从新组合，以解决更具有挑战性的任务。
Finn et al. 将此定义为 <em>meta learning</em>，并对智能体进行任务分配，这些任务被明确设计来适应该分布中一个新的任务，这几乎不需要额外的学习。
在线性强化学习设定中，当它们遇到时，通过策略梯度学习一个latent basis去解决新任务。
Brunskill and Li 得出了在终身学习中进行option discovery的样本复杂性的界限。</p>

<p>在这项工作中，我们的目标是使用 <code class="highlighter-rouge">single-stage end-to-end learning</code> 来解决具有深度依赖结构的任务。
此外，我们旨在所有任务上训练智能体而不考虑它们的复杂性。
然后，experience可以在任务之间共享，从而使得智能体能够有效地并行开发每个任务的能力。
为了实现这一点，我们需要将 <strong>task generalization</strong>（任务泛化）和 <strong>off-policy learning</strong> 结合起来。
有许多强化学习技术通过将任务直接合并到值函数的定义中来执行任务泛化。
<em>Universal Value Function Approximators (UVFAs)</em> 是这些工作的最新研究成果，它通过共享参数高效地将 a Horde of demons 组合到一个值函数中。
我们将UVFAs（它原来表现为两步学习）与off-policy goal learning相结合，更新到端到端的、最先进的并行智能体结构中，以实现持续学习。
这些components(组件)的新式组合产生了一个持续学习的智能体，我们称为 <code class="highlighter-rouge">Unicorn</code>，它能够大规模地学习具有深度依赖结构（Figure 1a, top）的non-trivial的任务。
独角兽智能体通过在任务之间共享experience、重用表示以及技能解决这些领域的相关问题，并且表现超过了baselines methods（Figure 1a, bottom）。
我们也证明了独角兽可以轻而易举地：(A) 解决多种任务（没有依赖关系）以及 (B) 当任务相关时表现出协同效应。</p>

<h3 id="background">Background</h3>
<p><strong>Reinforcement Learning</strong>(RL) 是一种计算框架，用于在不确定的序贯决策问题中做决策。
强化学习问题被描述为一个Markov decision process (MDP)，定义为五元组 $&lt;\matcal{S}, \mathcal{A}, r, \mathcal{P}, \gamma&gt;$，其中 $\mathcal{S}$ 表示状态集，$\mathcal{A}$ 表示动作集，
$r\ :\ \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ 表示奖励函数，$\mathcal{P}\ :\ \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ 表示转移概率分布，
$\gamma \in [0,1)$ 表示折扣系数。
策略 $\pi$ 映射状态 $s \in \mathcal{S}$ 到动作的概率分布。
我们定义给定时间步 $t$ 时的 <em>return</em> 为折扣奖励的和：$R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k r{t+k+1}$，其中 $r_t = r(s_t, a_t)$。
动作值函数 $Q^\pi(s,a) = \mathbb{E}^\pi [R_t|s_t = s, a_t = a]$ 估计一个智能体的 <em>return</em> 的期望值，该智能体在遵循策略 $\pi$ 之后，在一些状态 $s \in \mathcal{S}$ 下选择动作 $a \in \mathcal{A}$。
最优动作值函数 $Q^{<em>}(s,a)$ 估计的是基于最优策略 $\pi^{</em>}$ 时的 <em>return</em> 的期望。</p>

<p><strong>Q-learning</strong> 可以通过一个迭代引导过程来估计这个最优值函数 $Q^{*}(s,a)$，这其中 $Q(s_t,a_t)$ 朝着导向目标 $Z_t$ 更新，
$Z_t$ 是用下一个状态的估计Q值来构造的：$Z_t = r_{t+1} + \gamma \mathop{max}\limits_{a} Q(s_{t+1}, a)$。
$\delta_t = Z_t - Q(s_t, a_t)$ 表示的是时间差分误差(TD误差)。</p>
:ET