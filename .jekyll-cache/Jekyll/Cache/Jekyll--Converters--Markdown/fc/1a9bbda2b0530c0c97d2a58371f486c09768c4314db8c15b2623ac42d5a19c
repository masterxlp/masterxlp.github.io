I":w<ul id="markdown-toc">
  <li><a href="#简介" id="markdown-toc-简介">简介</a></li>
  <li><a href="#deep-q-learning" id="markdown-toc-deep-q-learning">Deep Q-Learning</a>    <ul>
      <li><a href="#背景" id="markdown-toc-背景">背景</a></li>
      <li><a href="#创新" id="markdown-toc-创新">创新</a>        <ul>
          <li><a href="#replay-buffer" id="markdown-toc-replay-buffer">Replay Buffer</a></li>
          <li><a href="#target-network" id="markdown-toc-target-network">Target Network</a></li>
        </ul>
      </li>
      <li><a href="#trick" id="markdown-toc-trick">Trick</a></li>
      <li><a href="#网络结构图" id="markdown-toc-网络结构图">网络结构图</a></li>
      <li><a href="#算法" id="markdown-toc-算法">算法</a></li>
      <li><a href="#延伸" id="markdown-toc-延伸">延伸</a>        <ul>
          <li><a href="#double-dqn" id="markdown-toc-double-dqn">Double DQN</a></li>
          <li><a href="#priority-replay-buffer" id="markdown-toc-priority-replay-buffer">Priority Replay Buffer</a></li>
          <li><a href="#dueling-dqn" id="markdown-toc-dueling-dqn">Dueling DQN</a></li>
          <li><a href="#解决dqn冷启动的问题" id="markdown-toc-解决dqn冷启动的问题">解决DQN冷启动的问题</a></li>
          <li><a href="#distributional-dqn" id="markdown-toc-distributional-dqn">Distributional DQN</a></li>
          <li><a href="#noisy-network" id="markdown-toc-noisy-network">Noisy Network</a></li>
          <li><a href="#rainbow" id="markdown-toc-rainbow">RainBow</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="简介">简介</h2>

<blockquote>
  <p>本部分包含对各种经典的强化学习算法的总结，包括但不限于DQN、DDPG、PPO、TRPO等算法。</p>
</blockquote>

<h2 id="deep-q-learning">Deep Q-Learning</h2>
<blockquote>
  <p>DQN 的基本思想就是：通过神经网络完成值函数的拟合。</p>
  <blockquote>
    <p>状态-动作值函数本质上计算了基于某个特定策略对未来长期回报的期望；</p>
  </blockquote>
</blockquote>

<h3 id="背景">背景</h3>
<p>DQN算法的提出是在以CNN为代表的深度学习在图像处理上取得显著效果之后。
在DQN提出之前，一直以来强化学习的主要方式表现为表格法和值函数近似法。
值函数近似又可分为线性值函数近似和非线性值函数近似。
深度神经网络属于非线性函数，且在图像处理上表现出了强大的拟合能力。
因此，强化学习与深度学习的结合的产物–深度强化学习，是必然的。</p>

<p>表格法受限于其对状态和动作空间大小的限制：当状态空间或动作空间变得极大时，会造成维度灾难；
而当状态空间或者动作空间无限时，表格法就无能为力了（直接表示是不可能的，只能通过其他类似于重构的方法来处理）。</p>

<p>值函数近似可以打破这种限制，它无所谓状态与动作的维度限制，若值函数拟合的好的话，那么对于任何状态与动作都可以通过表达式来计算出一个无限接近于真实值的近似值。
将值函数得到的值称为近似值是因为我们对于真实值的实际分布是什么样子的，近似值函数的本质就是通过一个函数来近似 Q 值 或者 V 值。</p>

<p>既然需要利用深度神经网络强大的拟合能力和表征能力，那么深度学习与强化学习的融合有哪些障碍呢？</p>
<ul>
  <li>标签（label）
    <ul>
      <li>普通深度学习属于监督学习，依赖于大量带label的样本数据；</li>
      <li>而强化学习只有奖励可以算是标签，但是在强化学习中奖励具有稀疏性、带有噪音且具有延迟的特性；</li>
    </ul>
  </li>
  <li>数据的独立性
    <ul>
      <li>深度学习用来训练的样本是要求相互独立的；</li>
      <li>而强化学习的样本是具有高度相关性的，具有鲜明的先后顺序；</li>
    </ul>
  </li>
  <li>数据分布的平稳性
    <ul>
      <li>深度学习要求数据服从平稳分布，即在训练过程中，数据服从的分布是不变的；</li>
      <li>而强化学习的数据分布会随着算法学习到的新的行为策略改变而改变；</li>
    </ul>
  </li>
</ul>

<p>那么，DQN是如何克服这些障碍的呢？</p>
<ul>
  <li>Replay Buffer</li>
  <li>Target Network</li>
</ul>

<h3 id="创新">创新</h3>

<blockquote>
  <p>在DQN之前，大多数强化学习问题都需要研究任务的领域知识（以提取特征等），同时需要解决同一序列内样本之间相互关联的问题，以及Q-learning算法中价值估计与更新值相互关联的问题。
作者通过 Replay Buffer、均匀随机采样、深度神经网络、目标网络等一系列设计和改进使得模型在Atari游戏上达到了优秀的效果，也为开启了强化学习和深度学习的结合之路。</p>
</blockquote>

<h4 id="replay-buffer">Replay Buffer</h4>
<p>Q-learning 方法基于当前策略进行交互和改进，更像是一种在线学习（Online Learning）的方法：每一次模型利用交互生成的数据进行学习，学习后的样本被直接丢弃。
但是当使用神经网络等机器学习模型代替表格式模型，依然采用这样的在线学习时，就会遇到问题：</p>
<ul>
  <li>交互得到的序列存在一定的相关性：交互序列中的状态、动作存在着一定的相关性，而对于基于最大似然法的机器学习模型来说，我们有一个重要的假设：训练样本是独立同分布的，一旦这个假设不成立，那么模型的
效果就会大打折扣；</li>
  <li>交互数据的使用效率：采用梯度下降法进行模型更新时，模型训练往往需要经过多轮迭代才能收敛，每一次迭代需要使用一定数量的样本计算梯度，如果每次计算的样本在计算一次梯度之后就被丢弃，那么我们需要花费更多
的时间与环境交互收集样本（对于一些一次交互需要花费较长时间的任务来说，这是致命的）；</li>
</ul>

<p>为了解决这个问题，作者提出了 Replay Buffer 这样的数据结构：Replay Buffer 中保存的是 agent 与 environment 交互产生的 <code class="highlighter-rouge">(s, a, r, s')</code> 这样的 transition。
它包含收集样本和采样样本两个过程：收集样本时按照时间先后顺序存入结构中，如果 Replay Buffer 存满了的话，就会从开始进行旧数据的覆盖（原则上是优先覆盖时间越久远的样本）；
采样样本的过程表现为在每次训练模型之前，在 Replay Buffer 中均匀的随机采样一批样本进行学习。</p>

<p>那么为什么使用均匀采用呢？它有什么好处呢？ <br />
前面提到我们交互得到的序列在时间维度上存在一定的相关性。
我们希望学习得到的 <strong>值函数</strong> 能够表示 <strong>在当前状态-动作下的长期收益的期望</strong>，然而每一次交互得到的序列，只能代表当前状态-动作下的一次采用轨迹，并不能代表所有可能的轨迹，这样估计的结果就和期望的结果存在一定的差距。
随着交互时间不断拉长，这个差距的累积就会越来越大。
如果完全使用序列的估计值进行训练，某一轮训练时模型会朝着一个序列的估计训练，另一轮训练又会朝着另一个序列的估计训练，那么模型就很容易产生较大的波动。
采用均匀采样后，每次训练的样本通常来自多次交互序列，这样单一序列的波动就被减轻很多，训练效果也就稳定了很多。
同时，一份样本也可以被多次训练，提高了样本的利用率。</p>

<p>事实上，总的来说，均匀的随机采样可以打破数据之间的关联性，间接满足训练所需求的独立同分布的特性，同分布的解释为：将所有的不同分布的历史数据放入 Replay Buffer 中，那么所有的不同分布将中和成为一个新的分布，
且所有数据均满足这个新的分布。</p>

<h4 id="target-network">Target Network</h4>

<blockquote>
  <p>Target Network 通过在一段时间内固定网络的参数，使得Q-learning方法的目标价值能够得到一定的固定，间接使得模型获得一定的稳定性。</p>
</blockquote>

<p>模型在训练过程中不稳定的另一个原因来自算法本身：从 Q-learning 的计算公式可以看出，算法可以分为如下两个步骤</p>
<ul>
  <li>计算当前状态-动作下的价值目标值：$\Delta q(s,a) = r(s’) + max_{a’} q^{T-1}(s’, a’)$；</li>
  <li>网络模型的更新：$q^{T}(s,a) = q^{T-1}(s,a) + \frac{1}{N} [\Delta q(s,a) - q^{T-1}(s,a)]$；</li>
</ul>

<p>可以看出模型通过当前时刻的回报和下一时刻的价值估计进行更新，这既像一场自导自演的电影，又像一场既当运动员又当裁判员的比赛。
这里存在一些隐患，前面提到数据样本差异可能造成一定的波动，由于数据本身存在着不稳定性，每一轮迭代都可能产生一些波动，如果按照上面的计算公式，这些波动会立刻反映到下一个迭代的计算中，这样我们就很难得到一个平稳的模型。
为了减轻相关问题到来的影响，我们要尽可能地将两个部分解藕。</p>

<p>为了能够缓解上面提到的波动性的问题，引入了目标网络，该模型与另一个结构完全一样，而原本的模型被称为表现模型（Behavior Network）。
两个模型的训练过程如下：</p>
<ul>
  <li>在训练开始时，两个模型使用完全相同的参数；</li>
  <li>在训练过程中，Behavior Network 负责与环境进行交互，得到交互样本；</li>
  <li>在学习过程中，Q-Learning 的目标价值是由 Target Network 计算得到，然后用它和 Behavior Network 的估计值进行比较得出 TD Error 并更新 Behavior Network；</li>
  <li>每当训练完成一定轮数的迭代，Behavior Network 模型的参数就会同步给 Target Network，这样就可以进行下一阶段的学习；</li>
</ul>

<p>通过使用 Target Network，计算目标价值的模型在一段时间内将被固定，这样就可以减轻模型的波动性。</p>

<h3 id="trick">Trick</h3>
<p>DQN中使用的技巧有：</p>
<ul>
  <li>Replay Buffer：提高样本利用率，打破数据的关联性以及非平稳分布的特性，实现数据的独立同分布；</li>
  <li>Target Network：在一段时间内固定目标值，减轻模型的波动性；</li>
  <li>值网络的架构：模型的架构很灵活，原论文中模型的输入为 <code class="highlighter-rouge">(s,a)</code> 输出单个 <code class="highlighter-rouge">Q(s,a)</code>，而 deepmind 在 DQN 的实现中，则是使用 <code class="highlighter-rouge">(s)</code> 作为模型输入，输出 $Q(s,a_1), Q(s, a_2), \cdots, Q(s, a_n)$；</li>
</ul>

<h3 id="网络结构图">网络结构图</h3>

<div align="center"><img src="../../../../image/DQN/DQN网络结构图.jpeg" /></div>

<h3 id="算法">算法</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DQN 算法
--------------------------------------------------------
Input
01. 初始化容量为 N 的 Replay Buffer D
02. 初始化状态-行为价值模型 Q 和参数 theta
03. 初始化 Target Network Q' 和参数 theta' = theta
Satrt
04. for episode = 1, M do
05.     初始化环境，得到初始状态 s_1，并预处理得到 \psi_1 = \psi_1(s_1) // 预处理指的是对图像大小的处理，论文中是将 210 x 160 的图像转换为了 84 x 84 的图像
06.     fot t = 1, T do
07.         以 epsilon 的概率随机选择一个动作 a_t，或者以 1 - epsilon 的概率根据模型选择当前最优的动作 a_t = max_a Q^{*}(\psi(s_t), a; theta)
08.         执行动作 a_t，得到新一轮的状态 s_{t+1} 和 回报 r_{t+1}
09.         预处理得到 \psi_{t+1} = \psi_{t+1}(s_{t+1})
10.         将 {\psi_{t}, a_t, r_{t+1}, \psi_{t+1}} 存储到 D 中
11.         从 D 中采样得到一批样本 (\psi_{j}, a_j, r_{j+1}, \psi_{j+1})
12.         计算 y_i = r_{j+1} for terminal \psi_{j+1}  r_{j+1} + gamma * max_a' Q(\psi_{j+1}, a'; theta')
13.         根据目标函数 (y_i - Q(\psi{j}, a_j; theta))^2 进行梯度下降，更新参数 theta
14.         每个 C 轮完成目标网络的参数更新 theta' = theta
15.     end for
16. end for
-------------------------------------------------------
</code></pre></div></div>

<h3 id="延伸">延伸</h3>
<h4 id="double-dqn">Double DQN</h4>
<blockquote>
  <p>通过优化目标 Q 值的计算来优化算法。</p>
  <blockquote>
    <p>为了减少模型的波动性，Double DQN 可以使用不同的模型完成最优行动的选择和价值估计两个工作，使价值估计的波动降低。
换句话说，DDQN 就是在 nature DQN 的基础上，通过解藕目标 Q 值动作的选择和目标 Q 值的估计这两步，来缓解过度估计的问题，降低值估计的波动性。</p>
  </blockquote>
</blockquote>

<p>在 Nature DQN 中，加入了 Target Network 来使得目标值在一段时间内被固定，以使模型获得一定的稳定性。
虽然这个方法提升了模型的稳定性，但是它并没有解决另外一个问题：Q-learning 对价值估计过高的问题。<br />
所谓的过度估计就是算法使得模型存在很大的偏差。
具体描述为：因为 Q-learning 在计算时利用了下一时刻的最优值，所以它通常在计算时给出了一个状态-行动的估计上限，由于训练过程中模型并不够稳定，因此对上限的估计存在一定的偏差；
如果偏差时一致的，也就是说，每个行动都拥有相近的偏差，那么偏差对模型效果的影响相对较小；如果偏差是不一致的，那么这个偏差会造成模型对行动优劣的判断偏差，这样就会影响模型的效果。</p>

<p>为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</p>

<p>我们已经知道 Target Network 求解价值目标值时的公式：</p>

\[\begin{align}
y_j = r_{j+1} + \gamma max_{a'} Q(s_{j+1}, a'; \theta^{-}) \tag{1}
\end{align}\]

<p>其中 $\theta^{-}$ 表示 Target Network 的参数。</p>

<p>将式（1）进一步展开，</p>

\[\begin{align}
y_j = r_{j+1} + \gamma Q(s_{j+1}, argmax_{a'} Q(s_{j+1}, a'; \theta^{-}); \theta^{-}) \tag{2}
\end{align}\]

<p>通过式（2）我们可以发现，在采用 target network 之后，模型在选择最优动作和计算目标值时依然使用了相同的参数模型，这样必然会造成对价值的过高估计。</p>

<p>为了尽可能的减少过高估计的影响，一个简单的办法就是把「选择最优动作」和「估计最优动作」两部分的工作分离，可以使用 behavior network 来完成最优动作的选择，这样就可以得到</p>

\[\begin{align}
y_j = r_{j+1} + \gamma Q(s_{j+1}, argmax_{a'} Q(s_{j+1}, a'; \theta); \theta^{-}) \tag{3}
\end{align}\]

<p>这样算法的三个环节模型的使用如下：</p>
<ul>
  <li>采样阶段：Behavior Network $Q(\theta)$</li>
  <li>选择最优动作阶段：Behavior Network $Q(\theta)$</li>
  <li>计算目标价值 $y$ 阶段：Target Network $Q(\theta^{-})$</li>
</ul>

<h4 id="priority-replay-buffer">Priority Replay Buffer</h4>
<blockquote>
  <p>通过优化经验回放池按权重采样来优化算法：</p>
</blockquote>

<h4 id="dueling-dqn">Dueling DQN</h4>
<blockquote>
  <p>通过优化神经网络结构来优化算法：</p>
  <blockquote>
    <p>它的主要突破点在于利用模型结构将值函数表示成更细致的形式，使得模型拥有更好的表现。</p>
  </blockquote>
</blockquote>

<p>基于状态-动作的值函数 Q 可以分解为基于状态的值函数 V 和「优势函数」A，即 $Q(s,a) = V(s) + A(s,a)$。
我们知道，状态-动作值函数 Q 和 状态值函数 V 存在这样的关系：$V(s) = E_{a \in A}[Q(s,a)]$，这意味着，状态值函数 V 为 状态-动作值函数 Q 的均值！
于是，当 优势值函数 A &gt; 0 时，表示当前动作的表现优于平均表现，反之，当前动作的表现比平均表现要差。</p>

<p>基于这样的概念，在设计如下的网络结构：</p>

<div align="center"><img src="../../../../image/DQN/DuelingDQN.png" /></div>

<p>它与 Double DQN 的区别仅在于将值网络的结构由一路输出变为了二路输出。
于是，Q 值的计算就变成了 $Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + A(s, a; \theta, \beta)$。
其中，$\theta$ 为公共部分网络的参数，$\alpha$ 为优势网络的参数，$\beta$ 为状态值网络的参数。</p>

<p>但是这样单纯的分解会存在一个问题：当 Q 值一定时，V 和 A 有无穷种可行的组合，但实际上只有一小部分的组合是合理的、接近真实数值的。
为了解决这个问题，我们需要对优势函数进行限定。
我们知道优势函数的期望值为 0：</p>

\[\begin{align}
E_a[A(s,a)] &amp;= E_a[Q(s, a) - V(s)] \\
&amp;= E_a[Q(s,a)] - V(s) \\
&amp;= V(s) - V(s) \\
&amp;= 0 \tag{4}
\end{align}\]

<p>因此，我们可以对输出的 A 值进行约束：</p>

\[\begin{align}
Q(s,a) = V(s) + (A(s,a) - \frac{1}{|A|} \sum_{a'} A(s, a')) \tag{5}
\end{align}\]

<p>让每一个 A 值减去当前状态下所有 A 值的平均数（实际上就是对优势值做中心化处理），就可以保证前面提到的期望为 0 的约束（括号内操作就是将所有的优势值零均值化），从而增加了 V 和 A 的输出稳定性。</p>

<p><strong>那么做这样的输出分解有什么好处呢？</strong></p>
<ul>
  <li>首先，通过这样的分解，我们不但可以得到给的状态和动作的 Q 值，还可以得到 V 值和 A 值。这样如果在某些场景下需要使用 V 值时，我们同样可以获得 V 值而不用再训练一个网络；同时，通过显式的给出 V 函数
的输出值，每一次更新时，我们也都会显式的更新 V 函数，这样 V 函数的更新频率就会得到确定性的增加，而对于单一输出的 Q 网络来说，它的更新就显得有些晦涩了；</li>
  <li>其次，从网络训练的角度来讲，我们从原本需要训练 $|A|$ 个取值在 $[0, +\infty]$ 的数值，变成了训练「一个」取值为 $[0, +\infty]$ 的数值和 $|A|$ 个均值为 0，实际取值为 $[-C, C]$ 的数值，对
网络训练来说，后者显然更友好、容易一些。对一些强化学习的问题来说，A 值的取值范围远比 V 值小，这样讲两者分开训练更容易保持行动之间的排列顺序；由于我们需要从所有的行动中挑选价值最高的行动，因此，不同的
行动之间的价值需要保持一定的区分度，由于 A 值的取值范围较小，因此它对模型的更新更加敏感，这样模型在更新时更容易考量与其他行动的相对变化量，也就不会因为某一次的更新使得原本的行动排序被意外打破。如果采用 Q
值进行更新，由于 Q 值相对于 A 值来说会很大，因此 Q 值对微小的更新不敏感，某一次更新可能会影响行动原本的排序，从而对策略造成一定的波动；</li>
  <li>最后，将值函数分解后，每一部分的结果都具有实际的意义，我们也可以从中挖掘出很大有价值的信息：论文中通过反卷积操作得到两个函数对原始输入图像的梯度后，得到 V 函数对游戏中所有关键信息都十分敏感，而 A 函数只对和
行动相关的信息敏感；</li>
</ul>

<h4 id="解决dqn冷启动的问题">解决DQN冷启动的问题</h4>
<blockquote>
  <p>一直以来，强化学习问题都会遇到一个冷启动的问题：对于以值函数为核心的 Q-learning 算法来说，前期算法的迭代很难让模型快速进入一个相对理想的环境，更何况由于前期值函数估计存在较大的偏差，与环境交互得到的采样
与最优策略存在一定的差别，这更增加了学习的难度。</p>
</blockquote>

<p>论文 Deep Q-learning from Demonstrations 提出了一种强化学习与监督学习相结合的一种方案。
其主要思路是：利用预先准备好的优质采样轨迹（例如，专家行为数据）加快模型前期的训练速度。</p>

<p>对于一个未经任何训练的模型来说，模仿优质采样轨迹的行动是一个不错的选择，因为模型训练初期 Agent 的策略相对较差，从交互序列中学习的效果不会很好，而优质行动本身来自较强的策略，这样就相当于站在巨人的肩膀上，学习速度自然会快很多。</p>

<p>除了监督学习的预训练外，我们还需要使用强化学习完成DQN模型中原本的训练，最终模型的学习目标函数变为：</p>

\[\begin{align}
J(Q) = J_{DQ}(Q) + \lambda_1 J_n(Q) + \lambda_2 J_E(Q) + \lambda_3 J_{L2}(Q) \tag{6}
\end{align}\]

<p>其中四个子目标的含义为</p>
<ul>
  <li>$J_{DQ} (Q)$：Deep Q-learning 的目标函数；</li>
  <li>$J_N (Q}$：以 n 步回报估计法为目标的 Q-learning 目标函数；</li>
  <li>$J_E(Q)$：利用准备数据进行监督训练的目标函数；</li>
  <li>$J_{L2}(Q)$：L2正则的目标函数；</li>
</ul>

<p>使用 n 步回报估计法是因为：对于序列较长的问题，一步估计法有一个缺陷，那就是价值需要一定时间的训练才能跳过前面波动较大的问题，进入相对稳定的更新期。
为了使模型更新的更快，采用 n 步回报估计：不仅使用下一时刻的回报，还使用更多时刻的回报加入目标值中。</p>

\[\begin{align}
Q(s,a) = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^{n-1} r_{t+n} + max_a \gamma^n Q(s_{t+n+1}, a) \tag{7}
\end{align}\]

<p>该方法结合了蒙特卡洛法的特点，在公式中使用更多真实的交互回报，这样更容易在模型训练早期将值函数做快速更新。</p>

<p>一般来说，事先准备好的数据是比较有限的，很难支撑一个完整模型的训练，因此它必然只能影响很小一部分的状态行动值。
如果它不能尽可能的覆盖更多的状态，那么这些数据反而有可能对模型造成不好的影响；同时，这些准备好的数据中可能也会存在一定的噪音，其中的行动并不是真正的行动。
因此，为了避免这些问题，监督学习的目标函数被定义为：</p>

\[\begin{align}
J_E(Q) = max_{a \in A}[Q(s,a) + l(a_E, a)] - Q(s, a_E) \tag{8}
\end{align}\]

<p>其中，$a_E$ 表示当前状态下专家给出的行动，$l(x,y)$ 函数是一个指示函数，当模型选择的动作与专家给出的动作一致时，函数值为0；否则为某个固定值。
当 $a = a_E$ 时，目标函数为 0 表示模型的决策和准备的数据相同；反之，则说明其他某个行动的价值至少不弱于专家的行动太多，这对模型来说是一个比较合理的约束。</p>

<p>文中提出的模型也使用了 target network、priority replay buffer 等技巧；</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>算法 Deep Q-learning from Demonstrations

Input
D : 专家行动数据作为初始化的 Replay Buffer
theta : Behavior Network 的参数
theta' : Target Netwoek 的参数
tau : 更新 target network 的频率
k : 预训练的轮数

Start
01. for steps t = 1, ..., k do
02.     从 D 中进行采样，得到一个 mini-batch 的数据
03.     使用 target network 计算损失函数 J(Q)
04.     对 behavior network 进行梯度下降并更新参数 theta
05.     当  t mod tau = 0 时，进行 target network 的更新：theta' = theta
06. for steps t = 1, 2, ... do
07.     利用 behavior network 与环境进行交互，获得执行数据 (s, a, r, s')
08.     将数据存入 D 中，同时根据容量情况替换老的数据
09.     从 D 中进行优先采样，得到一个 mini-batch 的数据
10.     使用 target network 计算损失函数 J(Q)
11.     对网络 theta 进行梯度下降并更新参数
12.     当  t mod tau = 0 时，进行 target network 的更新：theta' = theta
</code></pre></div></div>

<h4 id="distributional-dqn">Distributional DQN</h4>
<blockquote>
  <p>我们知道，状态-动作值函数对某个策略的未来长期回报的期望的估计，即不容易也不细致。
因此，Distributional DQN 模型希望通过建立更复杂细致的值函数来让估计结果更细致可信。</p>
</blockquote>

<blockquote>
  <p>这里不涉及理论推导，只解释原理以及思想过程，具体的理论推导可以参考<a href="https://zhuanlan.zhihu.com/p/137935717">知乎</a></p>
</blockquote>

<p>Q-learning 的贝尔曼公式：</p>

\[\begin{align}
Q(s, a) &amp;= r(s, a) + \sum_{s'} p(s' \mid s, a) \sum_{a'} \pi(a' \mid s') \gamma Q(s', a') \\
&amp;= r(s, a) + \sum_{s'} p(s' \mid s, a) \gamma E_{a' \sim \pi} [Q(s', a')] \\
&amp;= r(s, a) + \gamma E_{s' \sim p, a' \sim \pi} [Q(s', a')] \tag{9}
\end{align}\]

<p>在之前的 DQN 中，对于目标值的估计是基于策略和状态转移两个概率分布的价值期望。
而在 Distributional DQN 中，目标值的估计不再是对价值期望的估计而变成了对「价值分布」的估计。</p>

<p>换句话说，就是对每一个 $Q(s,a)$ 的分布做估计，那么最终的 $V(s)$ 就是这个分布的期望值。</p>

<p>由于需要对每一对的 <code class="highlighter-rouge">(s,a)</code> 做分布，这样它就会存在很多峰值，这样我们就不能用方差固定，估计均值的高斯分布了。
为了确保分布不受太多的限制，同时又可以减少分布的计算量，我们选择一种简单直观的方法：直方图。
我们只需要限定一个范围，使得绝大多数的价值采样点落在这个范围即可。</p>

<p>采用直方图的好处就是它可以近似很多不太常规的分布形式，分布的表示范围要比高斯分布大很多。</p>

<p>于是，我们有 $Q(s,a) = E[Z(s,a)]$，$Z(s,a)$ 表示了采样点的位置。
这样模型的输入输出就可以确定了：模型的输入依然只是状态 $s$，输出则是一个矩阵，矩阵的每一行表示一个动作的各个采样点的概率，矩阵的每一列的值表示了每个动作在各个采样点的值。</p>

<p>由于我们采用的是直方图，于是我们每个采样点的值可以计算得到：</p>

\[\begin{align}
\Delta z &amp;= \frac{V_{MAX} - V_{MIN}}{N - 1} \tag{10} \\
z_i &amp;= V_{MIN} + i \Delta z \ :\  i \in [0, N) \tag{11}
\end{align}\]

<p>于是，对于每一个动作 $a_j$，有 $Q(s,a_j) = \sum_i z_i p_i (s,a_j)$。
其中，$p_i$ 是直方图模型通过 softmax 层输出每一个价值采样点的概率得到的。</p>

<p>接下来就重要的就是模型的更新，也就是说，模型的损失函数是什么？只要知道了损失函数就可以通过梯度下降来更新模型了。</p>

<p>我们一步一步来推进：首先，更新模型时我们需要在 Replay Buffer 中采样数据，<code class="highlighter-rouge">(s, a, r, s')</code>，接下来，我们需要计算目标值 $y :\ y = r + \gamma Z(s’, a^{<em>})$。<br />
那么第一个问题来了，$a^{</em>}$ 从哪里得到？<br />
因为 $Z(s,a)$ 是分布的分布，我们不好比较同一个 $s$ 下那个动作 $a$ 更好，所以我们从 $Q(s’, a)$ 中选择一个最优的 $a^{*}$：</p>

\[\begin{align}
Q(s_{t+1}, a) &amp;= \sum_i z_i p_i(s_{t+1}, a) \tag{12} \\
a^{*} &amp;\leftarrow argmax_{a} Q(s_{t+1}, a) \tag{13}
\end{align}\]

<p>于是我们就得到了目标值的一组 N 维向量，该向量表示的是一个分布，记为 $p’$：</p>
<ul>
  <li>有 $p_{0}^{‘}$ 的概率取 $r + \gamma z_0$</li>
  <li>有 $p_{1}^{‘}$ 的概率取 $r + \gamma z_1$</li>
  <li>$\cdots$</li>
  <li>有 $p_{N-1}^{‘}$ 的概率取 $r + \gamma z_{N-1}$</li>
</ul>

<p>我们也得到一组 $Z(s,a)$ 的 N 维向量，也表示一个向量，表示当前值，记为 $p$：</p>
<ul>
  <li>有 $p_0$ 的概率取 $z_0$</li>
  <li>有 $p_1$ 的概率取 $z_1$</li>
  <li>$\cdots$</li>
  <li>有 $p_{N-1}$ 的概率取 $z_{N-1}$</li>
</ul>

<p>显然，$z_i$ 的值没有对齐，目标值相对于当前值不只是价值范围发生了偏移，就连价值采样点也发生了偏移。</p>

<p>这样的便宜不允许我们使用交叉熵损失。</p>

<p>为了不使模型变得复杂，最简单的办法就是对齐采样点和价值范围。</p>

<p>具体的，就是把 $r + \gamma z_0$ 分摊到 $z_0$ 和 $z_1$ 上，把 $r + \gamma z_1$ 分摊到 $z_1$ 和 $z_2$ 上，以此类推。
分摊的方式是：按照 $r + \gamma z_0$ 到 $z_0$ 和 $z_1$ 的距离的反比就可以。</p>

<div align="center"><img src="../../../../image/DQN/DDQN模型更新图.jpg" /></div>

<h4 id="noisy-network">Noisy Network</h4>

<h4 id="rainbow">RainBow</h4>

<p>RadinBow 模型其实是集合了以上所有模型的特点：</p>
<ul>
  <li>Double DQN 的关于目标值估计和最优动作选择解藕的优势：$L(\theta) = (r_t + \gamma Q(s_{t+1}, argmax_{a’} Q(s_{t+1}, a’; \theta); \theta^{-}) - Q(s_t, a_t; \theta))^2$；</li>
  <li>Priority Replay Buffer</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Dueling DQN 的关于值函数网络分解的优势：$Q(s_t, a_t) = V(s_t) + (A(s_t, a_t) - \frac{1}{</td>
          <td>A</td>
          <td>} \sum_{a’} A(s_t, a’))$；</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>关于DQN的冷启动：利用优质的数据通过监督学习的方式在前期对模型进行预训练，加速模型的收敛；</li>
  <li>Multi-step Learning 的 n 步回报，来加速模型提前进入稳定期，加快模型收敛：$Q(s_t, a_t) = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^{n-1} r_{t+n} + \gamma^{n} max_a Q(s_{t+1}, a)$；</li>
  <li>Distributional DQN 的值分布形式，提升模型的表现力：${z_i = V_{MIN} + i \Delta z :\ 0 &lt;= i &lt; N}$, $p_i(s, a) = \frac{e^{\theta_i(s,a)}}{\sum_j e^{\theta_j(s,a)}}$；</li>
  <li>Noisy Network 的平滑、灵活的探索能力：$L(\theta) = E_{\epsilon^{-}, \epsilon}[E_{(s_t, a_t, r_t, s_{t+1}) \sim D} [r_t + \gamma max_{a^{<em>} \in A} Q(s_{t+1}, a^{</em>}, \epsilon^{-}; \theta^{-}, \sigma^{-}) - Q(s_t, a_t, \epsilon; \theta, \sigma)]^2]$；</li>
</ul>

:ET