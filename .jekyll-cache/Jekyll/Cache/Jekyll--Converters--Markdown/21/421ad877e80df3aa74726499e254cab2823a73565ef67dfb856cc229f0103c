I"o+<ul id="markdown-toc">
  <li><a href="#简介" id="markdown-toc-简介">简介</a></li>
  <li><a href="#deep-q-learning" id="markdown-toc-deep-q-learning">Deep Q-Learning</a>    <ul>
      <li><a href="#背景" id="markdown-toc-背景">背景</a></li>
      <li><a href="#改进" id="markdown-toc-改进">改进</a>        <ul>
          <li><a href="#replay-buffer" id="markdown-toc-replay-buffer">Replay Buffer</a></li>
          <li><a href="#target-network" id="markdown-toc-target-network">Target Network</a></li>
        </ul>
      </li>
      <li><a href="#trick" id="markdown-toc-trick">Trick</a></li>
      <li><a href="#算法" id="markdown-toc-算法">算法</a></li>
      <li><a href="#延伸" id="markdown-toc-延伸">延伸</a>        <ul>
          <li><a href="#double-dqn" id="markdown-toc-double-dqn">Double DQN</a></li>
          <li><a href="#dueling-dqn" id="markdown-toc-dueling-dqn">Dueling DQN</a></li>
          <li><a href="#rainbow" id="markdown-toc-rainbow">RainBow</a></li>
          <li><a href="#distributional-dqn" id="markdown-toc-distributional-dqn">Distributional DQN</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="简介">简介</h2>

<blockquote>
  <p>本部分包含对各种经典的强化学习算法的总结，包括但不限于DQN、DDPG、PPO、TRPO等算法。</p>
</blockquote>

<h2 id="deep-q-learning">Deep Q-Learning</h2>
<h3 id="背景">背景</h3>
<p>DQN算法的提出是在以CNN为代表的深度学习在图像处理上取得显著效果之后。
在DQN提出之前，一直以来强化学习的主要方式表现为表格法和值函数近似法。
值函数近似又可分为线性值函数近似和非线性值函数近似。
深度神经网络属于非线性函数，且在图像处理上表现出了强大的拟合能力。
因此，强化学习与深度学习的结合的产物–深度强化学习，是必然的。</p>

<p>表格法受限于其对状态和动作空间大小的限制：当状态空间或动作空间变得极大时，会造成维度灾难；
而当状态空间或者动作空间无限时，表格法就无能为力了（直接表示是不可能的，只能通过其他类似于重构的方法来处理）。</p>

<p>值函数近似可以打破这种限制，它无所谓状态与动作的维度限制，若值函数拟合的好的话，那么对于任何状态与动作都可以通过表达式来计算出一个无限接近于真实值的近似值。
将值函数得到的值称为近似值是因为我们对于真实值的实际分布是什么样子的，近似值函数的本质就是通过一个函数来近似 Q 值 或者 V 值。</p>

<p>既然需要利用深度神经网络强大的拟合能力和表征能力，那么深度学习与强化学习的融合有哪些障碍呢？</p>
<ul>
  <li>标签（label）
    <ul>
      <li>普通深度学习属于监督学习，依赖于大量带label的样本数据；</li>
      <li>而强化学习只有奖励可以算是标签，但是在强化学习中奖励具有稀疏性、带有噪音且具有延迟的特性；</li>
    </ul>
  </li>
  <li>数据的独立性
    <ul>
      <li>深度学习用来训练的样本是要求相互独立的；</li>
      <li>而强化学习的样本是具有高度相关性的，具有鲜明的先后顺序；</li>
    </ul>
  </li>
  <li>数据分布的平稳性
    <ul>
      <li>深度学习要求数据服从平稳分布，即在训练过程中，数据服从的分布是不变的；</li>
      <li>而强化学习的数据分布会随着算法学习到的新的行为策略改变而改变；</li>
    </ul>
  </li>
</ul>

<p>那么，DQN是如何克服这些障碍的呢？</p>
<ul>
  <li>Replay Buffer</li>
  <li>Target Network</li>
</ul>

<h3 id="改进">改进</h3>
<h4 id="replay-buffer">Replay Buffer</h4>
<p>Q-learning 方法基于当前策略进行交互和改进，更像是一种在线学习（Online Learning）的方法：每一次模型利用交互生成的数据进行学习，学习后的样本被直接丢弃。
但是当使用神经网络等机器学习模型代替表格式模型，依然采用这样的在线学习时，就会遇到问题：</p>
<ul>
  <li>交互得到的序列存在一定的相关性：交互序列中的状态、动作存在着一定的相关性，而对于基于最大似然法的机器学习模型来说，我们有一个重要的假设：训练样本是独立同分布的，一旦这个假设不成立，那么模型的
效果就会大打折扣；</li>
  <li>交互数据的使用效率：采用梯度下降法进行模型更新时，模型训练往往需要经过多轮迭代才能收敛，每一次迭代需要使用一定数量的样本计算梯度，如果每次计算的样本在计算一次梯度之后就被丢弃，那么我们需要花费更多
的时间与环境交互收集样本（对于一些一次交互需要花费较长时间的任务来说，这是致命的）；</li>
</ul>

<p>为了解决这个问题，作者提出了 Replay Buffer 这样的数据结构：Replay Buffer 中保存的是 agent 与 environment 交互产生的 <code class="highlighter-rouge">(s, a, r, s')</code> 这样的 transition。
它包含收集样本和采样样本两个过程：收集样本时按照时间先后顺序存入结构中，如果 Replay Buffer 存满了的话，就会从开始进行旧数据的覆盖（原则上是优先覆盖时间越久远的样本）；
采样样本的过程表现为在每次训练模型之前，在 Replay Buffer 中均匀的随机采样一批样本进行学习。</p>

<p>那么为什么使用均匀采用呢？它有什么好处呢？ <br />
前面提到我们交互得到的序列在时间维度上存在一定的相关性。
我们希望学习得到的 <strong>值函数</strong> 能够表示 <strong>在当前状态-动作下的长期收益的期望</strong>，然而每一次交互得到的序列，只能代表当前状态-动作下的一次采用轨迹，并不能代表所有可能的轨迹，这样估计的结果就和期望的结果存在一定的差距。
随着交互时间不断拉长，这个差距的累积就会越来越大。
如果完全使用序列的估计值进行训练，某一轮训练时模型会朝着一个序列的估计训练，另一轮训练又会朝着另一个序列的估计训练，那么模型就很容易产生较大的波动。
采用均匀采样后，每次训练的样本通常来自多次交互序列，这样单一序列的波动就被减轻很多，训练效果也就稳定了很多。
同时，一份样本也可以被多次训练，提高了样本的利用率。</p>

<p>事实上，总的来说，均匀的随机采样可以打破数据之间的关联性，间接满足训练所需求的独立同分布的特性，同分布的解释为：将所有的不同分布的历史数据放入 Replay Buffer 中，那么所有的不同分布将中和成为一个新的分布，
且所有数据均满足这个新的分布。</p>

<h4 id="target-network">Target Network</h4>
<p>模型在训练过程中不稳定的另一个原因来自算法本身：从 Q-learning 的计算公式可以看出，算法可以分为如下两个步骤</p>
<ul>
  <li>计算当前状态-动作下的价值目标值：$\Delta q(s,a) = r(s’) + max_{a’} q^{T-1}(s’, a’)$；</li>
  <li>网络模型的更新：$q^{T}(s,a) = q^{T-1}(s,a) + \frac{1}{N} [\Delta q(s,a) - q^{T-1}(s,a)]$；</li>
</ul>

<p>可以看出模型通过当前时刻的回报和下一时刻的价值估计进行更新，这既像一场自导自演的电影，又像一场既当运动员又当裁判员的比赛。
这里存在一些隐患，前面提到数据样本差异可能造成一定的波动，由于数据本身存在着不稳定性，每一轮迭代都可能产生一些波动，如果按照上面的计算公式，这些波动会立刻反映到下一个迭代的计算中，这样我们就很难得到一个平稳的模型。
为了减轻相关问题到来的影响，我们要尽可能地将两个部分解藕。</p>

<p>为了能够缓解上面提到的波动性的问题，引入了目标网络，该模型与另一个结构完全一样，而原本的模型被称为表现模型（Behavior Network）。
两个模型的训练过程如下：</p>
<ul>
  <li>在训练开始时，两个模型使用完全相同的参数；</li>
  <li>在训练过程中，Behavior Network 负责与环境进行交互，得到交互样本；</li>
  <li>在学习过程中，Q-Learning 的目标价值是由 Target Network 计算得到，然后用它和 Behavior Network 的估计值进行比较得出 TD Error 并更新 Behavior Network；</li>
  <li>每当训练完成一定轮数的迭代，Behavior Network 模型的参数就会同步给 Target Network，这样就可以进行下一阶段的学习；</li>
</ul>

<p>通过使用 Target Network，计算目标价值的模型在一段时间内将被固定，这样就可以减轻模型的波动性。</p>

<h3 id="trick">Trick</h3>
<p>DQN中使用的技巧有：</p>
<ul>
  <li>Replay Buffer：提高样本利用率，打破数据的关联性以及非平稳分布的特性，实现数据的独立同分布；</li>
  <li>Target Network：在一段时间内固定目标值，减轻模型的波动性；</li>
  <li>值网络的架构：模型的架构很灵活，原论文中模型的输入为 <code class="highlighter-rouge">(s,a)</code> 输出单个 <code class="highlighter-rouge">Q(s,a)</code>，而 deepmind 在 DQN 的实现中，则是使用 <code class="highlighter-rouge">(s)</code> 作为模型输入，输出 $Q(s,a_1), Q(s, a_2), \cdots, Q(s, a_n)$；</li>
</ul>

<h3 id="算法">算法</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DQN 算法
--------------------------------------------------------
Input
01. 初始化容量为 N 的 Replay Buffer D
02. 初始化状态-行为价值模型 Q 和参数 theta
03. 初始化 Target Network Q' 和参数 theta' = theta
Satrt
04. for episode = 1, M do
05.     初始化环境，得到初始状态 s_1，并预处理得到 \psi_1 = \psi_1(s_1)
06.     fot t = 1, T do
07.         以 epsilon 的概率随机选择一个动作 a_t，或者以 1 - epsilon 的概率根据模型选择当前最优的动作 a_t = max_a Q^{*}(\psi(s_t), a; theta)
08.         执行动作 a_t，得到新一轮的状态 s_{t+1} 和 回报 r_{t+1}
09.         预处理得到 \psi_{t+1} = \psi_{t+1}(s_{t+1})
10.         将 {\psi_{t}, a_t, r_{t+1}, \psi_{t+1}} 存储到 D 中
11.         从 D 中采样得到一批样本 (\psi_{j}, a_j, r_{j+1}, \psi_{j+1})
12.         计算 y_i = r_{j+1} for terminal \psi_{j+1}  r_{j+1} + gamma * max_a' Q(\psi_{j+1}, a'; theta')
13.         根据目标函数 (y_i - Q(\psi{j}, a_j; theta))^2 进行梯度下降，更新参数 theta
14.         每个 C 轮完成目标网络的参数更新 theta' = theta
15.     end for
16. end for
-------------------------------------------------------
</code></pre></div></div>

<h3 id="延伸">延伸</h3>
<h4 id="double-dqn">Double DQN</h4>

<h4 id="dueling-dqn">Dueling DQN</h4>

<h4 id="rainbow">RainBow</h4>

<h4 id="distributional-dqn">Distributional DQN</h4>

:ET