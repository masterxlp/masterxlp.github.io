I"<ul id="markdown-toc">
  <li><a href="#blogs" id="markdown-toc-blogs">Blogs</a>    <ul>
      <li><a href="#multi-task" id="markdown-toc-multi-task">Multi-Task</a></li>
    </ul>
  </li>
  <li><a href="#paper" id="markdown-toc-paper">Paper</a>    <ul>
      <li><a href="#multi-task-1" id="markdown-toc-multi-task-1">Multi-Task</a></li>
      <li><a href="#deepmind" id="markdown-toc-deepmind">DeepMind</a></li>
    </ul>
  </li>
  <li><a href="#comments" id="markdown-toc-comments">Comments</a></li>
</ul>

<h2 id="blogs">Blogs</h2>
<h3 id="multi-task">Multi-Task</h3>

<ul>
  <li><a href="https://mp.weixin.qq.com/s/DSDkksVM89gZsbP37kpG3Q?">多任务介绍</a><br />
Deep Learning 回顾之多任务学习</li>
</ul>

<h2 id="paper">Paper</h2>
<h3 id="multi-task-1">Multi-Task</h3>
<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1802.08294">Unicorn: Continual learning with a universal, off-policy agent</a><br />
UVFA在多任务中的应用 + 终身学习</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.03300">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a><br />
DDPG + Multi-task</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.04175">Distral: Robust Multitask Reinforcement Learning</a> <br />
Distill &amp; transfer learning</p>
  </li>
  <li>
    <p><a href="http://proceedings.mlr.press/v37/schaul15.pdf">Universal Value Function Approximators</a><br />
通用值函数近似，对传统的值函数进行扩展，添加了目标作为输入</p>
  </li>
</ul>

<h3 id="deepmind">DeepMind</h3>
<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1509.06461.pdf">2015 - Deep Reinforcement Learning with Double Q-learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1507.04296.pdf">2015 - Massively Parallel Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1510.09142.pdf">2015 - Learning Continuous Control Policies by Stochastic Value Gradients</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1509.08731">2015 - Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1512.04860">2016 - Increasing the Action Gap–New Operators for Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.03348">2016 - Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1610.05182">2016 - Learning and Transfer of Modulated Locomotor Controllers</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.04460">2016 - Model-Free Episodic Control</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.06295">2016 - POLICY DISTILLATION</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.04671">2016 - Progressive Neural Networks</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1509.02971">2016 - Continuous control with deep reinforcement learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.05952">2016 - PRIORITIZED EXPERIENCE REPLAY</a><br />
<code class="highlighter-rouge">优先级经验回放</code></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.01783">2016 - Asynchronous Methods for Deep Reinforcement Learning</a><br />
<code class="highlighter-rouge">A3C</code> – 深度强化学习的异步方法</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1603.00748">2016 - Continuous Deep Q-Learning with Model-based Acceleration</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.06581">2016 - Dueling Network Architectures for Deep Reinforcement Learning</a><br />
<code class="highlighter-rouge">Dueling</code> - 深度强化学习的对抗神经网络结构</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.04621">2016 - Deep Exploration via Bootstrapped DQN</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1605.06676">2016 - Learning to Communicate with Deep Multi-Agent Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.07714">2016 - Learning values across many orders of magnitude</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.04695">2016 - Strategic Attentive Writer for Learning Macro-Actions</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.01868">2016 - Unifying Count-Based Exploration and Intrinsic Motivation</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1610.01945">2017 - Connecting Generative Adversarial Networks and Actor-Critic Methods</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.02286">2017 - Emergence of Locomotion Behaviours in Rich Environments</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.02201">2017 - Learning human behaviors from motion capture by adversarial imitation</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.06170">2017 - Learning model-based planning from scratch</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1611.05763">2017 - LEARNING TO REINFORCEMENT LEARN</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.08817">2017 - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.03300">2017 - The Intentional Unintentional Agent–Learning to Solve Many Continuous Control Tasks Simultaneously</a></p>
  </li>
  <li>
    <p><a href="">2017 - COMBINING POLICY GRADIENT AND Q-LEARNING</a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
  <li>
    <p><a href=""></a></p>
  </li>
</ul>

<h2 id="comments">Comments</h2>

:ET