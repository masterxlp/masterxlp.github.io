I"s4<ul id="markdown-toc">
  <li><a href="#blogs" id="markdown-toc-blogs">Blogs</a>    <ul>
      <li><a href="#工具" id="markdown-toc-工具">工具</a></li>
      <li><a href="#multi-task" id="markdown-toc-multi-task">Multi-Task</a></li>
    </ul>
  </li>
  <li><a href="#paper" id="markdown-toc-paper">Paper</a>    <ul>
      <li><a href="#multi-task-1" id="markdown-toc-multi-task-1">Multi-Task</a></li>
      <li><a href="#multi-objective" id="markdown-toc-multi-objective">Multi-objective</a></li>
      <li><a href="#on-policy-to-off-policy" id="markdown-toc-on-policy-to-off-policy">On-policy to Off-policy</a></li>
      <li><a href="#network-structure" id="markdown-toc-network-structure">network structure</a></li>
      <li><a href="#deepmind" id="markdown-toc-deepmind">DeepMind</a></li>
    </ul>
  </li>
  <li><a href="#comments" id="markdown-toc-comments">Comments</a></li>
</ul>

<h2 id="blogs">Blogs</h2>
<h3 id="工具">工具</h3>
<ul>
  <li><a href="https://www.jianshu.com/p/93ccc63e5a1b">markdown编写数学公式</a></li>
</ul>

<h3 id="multi-task">Multi-Task</h3>

<ul>
  <li>
    <p><a href="https://mp.weixin.qq.com/s/DSDkksVM89gZsbP37kpG3Q?">多任务介绍</a><br />
Deep Learning 回顾之多任务学习</p>
  </li>
  <li>
    <p><a href="https://mp.weixin.qq.com/s/XHdaoOWBgOWX7SrOemY4jw">直接特征预测</a><br />
多目标学习 + <strong>直接特征预测</strong> + 无人机送货 + 项目实战 + 监督学习 + <a href="https://arxiv.org/abs/1611.01779">参考文章：Learning to Act by Predicting the Future</a></p>
  </li>
</ul>

<h2 id="paper">Paper</h2>
<h3 id="multi-task-1">Multi-Task</h3>
<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1802.08294">Unicorn: Continual learning with a universal, off-policy agent</a><br />
<strong>UVFA</strong> + <strong>end-to-end</strong> + 终身学习</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.03300">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a><br />
DDPG + Multi-task</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.04175">Distral: Robust Multitask Reinforcement Learning</a> <br />
Distill &amp; transfer learning</p>
  </li>
  <li>
    <p><a href="http://proceedings.mlr.press/v37/schaul15.pdf">Universal Value Function Approximators</a><br />
<strong>UVFA</strong> + 通用值函数近似 + <strong>Horde-based</strong> + <strong>bootstrapping</strong> + <strong>监督学习</strong></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2002.06038">NEVER GIVE UP: LEARNING DIRECTED EXPLORATION STRATEGIES</a><br />
UVFA的扩展使用</p>
  </li>
  <li>
    <p><a href="http://tongzhang-ml.org/papers/iclr19-dher.pdf">DHER: hindsight experience replay for dynamic goals</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1809.04474">Multi-task Deep Reinforcement Learning with PopArt</a>
对多个任务中的奖励使用PopArt技术进行裁剪，使得智能体不会偏向于奖励高的任务</p>
  </li>
</ul>

<h3 id="multi-objective">Multi-objective</h3>
<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1803.02965">2018 - A Multi-Objective Deep Reinforcement Learning Framework</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1809.07803">2019 - Dynamic Weights in Multi-Objective Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="http://jmlr.org/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf">2014 - Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1610.02707">2016 - Multi-Objective Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1402.0590">2014 - A Survey of Multi-Objective Sequential Decision-Making</a></p>
  </li>
</ul>

<h3 id="on-policy-to-off-policy">On-policy to Off-policy</h3>

<ul>
  <li><a href="https://arxiv.org/abs/1802.01561">2018 - IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a> <br />
<code class="highlighter-rouge">重要性权重</code></li>
</ul>

<h3 id="network-structure">network structure</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1511.06581">深度强化学习中的对抗网络结构</a></li>
</ul>

<h3 id="deepmind">DeepMind</h3>
<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1509.06461.pdf">2015 - Deep Reinforcement Learning with Double Q-learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1507.04296.pdf">2015 - Massively Parallel Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1510.09142.pdf">2015 - Learning Continuous Control Policies by Stochastic Value Gradients</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1509.08731">2015 - Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1512.04860">2016 - Increasing the Action Gap: New Operators for Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.03348">2016 - Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1610.05182">2016 - Learning and Transfer of Modulated Locomotor Controllers</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.04460">2016 - Model-Free Episodic Control</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.06295">2016 - POLICY DISTILLATION</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.04671">2016 - Progressive Neural Networks</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1509.02971">2016 - Continuous control with deep reinforcement learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.05952">2016 - PRIORITIZED EXPERIENCE REPLAY</a><br />
<code class="highlighter-rouge">优先级经验回放</code></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.01783">2016 - Asynchronous Methods for Deep Reinforcement Learning</a><br />
<code class="highlighter-rouge">A3C</code> – 深度强化学习的异步方法</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1603.00748">2016 - Continuous Deep Q-Learning with Model-based Acceleration</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.06581">2016 - Dueling Network Architectures for Deep Reinforcement Learning</a><br />
<code class="highlighter-rouge">Dueling</code> - 深度强化学习的对抗神经网络结构</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.04621">2016 - Deep Exploration via Bootstrapped DQN</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1605.06676">2016 - Learning to Communicate with Deep Multi-Agent Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1602.07714">2016 - Learning values across many orders of magnitude</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.04695">2016 - Strategic Attentive Writer for Learning Macro-Actions</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.01868">2016 - Unifying Count-Based Exploration and Intrinsic Motivation</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1610.01945">2017 - Connecting Generative Adversarial Networks and Actor-Critic Methods</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.02286">2017 - Emergence of Locomotion Behaviours in Rich Environments</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.02201">2017 - Learning human behaviors from motion capture by adversarial imitation</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.06170">2017 - Learning model-based planning from scratch</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1611.05763">2017 - LEARNING TO REINFORCEMENT LEARN</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.08817">2017 - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.03300">2017 - The Intentional Unintentional Agent–Learning to Solve Many Continuous Control Tasks Simultaneously</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1611.01626">2017 - COMBINING POLICY GRADIENT AND Q-LEARNING</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1704.02254">2017 - RECURRENT ENVIRONMENT SIMULATORS</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1611.01224">2017 - SAMPLE EFFICIENT ACTOR-CRITIC WITH EXPERIENCE REPLAY</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.06887">2107 - A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1703.01310">2017 - Count-Based Exploration with Neural Density Models</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.08475">2017 - DARLA: Improving Zero-Shot Transfer in Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1703.01161">2017 - FeUdal Networks for Hierarchical Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.06600">2017 - A multi-agent reinforcement learning model of common-pool resource appropriation</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1711.00832">2017 - A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.04175">2017 - Distral: Robust Multitask Reinforcement Learning</a><br />
<code class="highlighter-rouge">policy distill</code> 策略蒸馏</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.06203">2017 - Imagination-Augmented Agents for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.06383">2017 - Programmable Agents</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1707.02747">2017 - Robust Imitation of Diverse Behaviors</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.05312">2017 - Successor Features for Transfer in Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1611.01843">2017 - LEARNING TO PERFORM PHYSICS EXPERIMENTS VIA DEEP REINFORCEMENT LEARNING</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1611.05397">2017 - REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1710.02298">2018 - Rainbow- Combining Improvements in Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.05296">2018 - Value-Decomposition Networks For Cooperative Multi-Agent Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1803.03835">2018 - Kickstarting Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.11593">2018 - Observe and Look Further: Achieving Consistent Performance on Atari</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1801.08757">2018 - Safe Exploration in Continuous Action Spaces</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.08294">2018 - Unicorn- Continual learning with a universal, off-policy agent</a><br />
<code class="highlighter-rouge">UVFA</code> 的另一种结构 <code class="highlighter-rouge">Continual Learning</code> 终身学习</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.10295">2018 - NOISY NETWORKS FOR EXPLORATION</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.04697">2018 - Learning to Search with MCTSnets</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1806.01780">2018 - Mix &amp; Match – Agent Curricula for Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.03493">2018 - More Robust Doubly Robust Off-policy Evaluation</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.06370">2018 - Progress &amp; Compress: A scalable framework for continual learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1709.05380">2018 - The Uncertainty Bellman Equation and Exploration</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.09801">2018 - Meta-Gradient Reinforcement Learning</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.11592">2018 - Playing hard exploration games by watching YouTube</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1806.01822">2018 - Relational recurrent neural networks</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.10567">2018 - Learning by Playing – Solving Sparse Reward Tasks from Scratch</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.09564">2018 - Reinforcement and Imitation Learning for Diverse Visuomotor Skills</a></p>
  </li>
  <li>
    <p><a href="https://openreview.net/pdf?id=rJlJ-2CqtX">2019 - SUCCESS AT ANY COST: VALUE CONSTRAINED MODEL-FREE CONTINUOUS CONTROL</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1902.07151">2019 - EMERGENT COORDINATION THROUGH COMPETITION</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1806.01946">2019 - LEARNING TO UNDERSTAND GOAL SPECIFICATIONS BY MODELLING REWARD</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.11199">2019 - Value Propagation Networks</a></p>
  </li>
</ul>

<h2 id="comments">Comments</h2>

:ET