I"¨<h2 id="proximal-policy-optimization-algorithms">Proximal Policy Optimization Algorithms</h2>
<h3 id="è„‰ç»œåˆ†æ">è„‰ç»œåˆ†æ</h3>
<p>PPOè®ºæ–‡å®é™…ä¸Šæ—¶æå‡ºäº†ä¸€ä¸ª <code class="highlighter-rouge">clipped surrogate objective</code>ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜è¯¥ç›®æ ‡å‡½æ•°è¡¨ç°ç¡®å®ä¸é”™ï¼ˆå®é™…ä¸Šè¿˜æ˜¯åˆ†åœºæ™¯çš„ï¼‰ã€‚</p>

<p>ä» <strong>Policy Gradient</strong> å¼€å§‹ï¼Œå…¶ç›®æ ‡å‡½æ•°ä¸º $L^{PG}(\theta) = \hat{\mathbb{E}}<em>t [log\pi</em>{\theta}(a_t|s_t)\hat{A}<em>t]$ã€‚
ç›´è§‚åœ°ï¼Œå…¶ç­–ç•¥æ›´æ–°ä¾èµ–äº <strong>ç­–ç•¥</strong> $\pi(\theta)$ ä»¥åŠ <strong>ä¼˜åŠ¿å€¼</strong>  $\hat{A}_t$ã€‚
äº‹å®ä¸Šï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ›´æ–°æ–¹å¼ä¸ºï¼šä¾æ®äºå½“å‰çš„ç­–ç•¥ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ”¶é›†ä¸€æ‰¹æ•°æ®ï¼ŒåŸºäºè¿™æ‰¹æ•°æ®æ‰§è¡Œä¸€æ¬¡ç­–ç•¥æ›´æ–°ã€‚
å› æ­¤ï¼Œå®ƒå±äº <strong>on-policy</strong> çš„ç®—æ³•ã€‚
å¯¹äºon-policyçš„æ–¹æ³•ï¼Œå¦‚æœåˆ©ç”¨â€œæ—§â€æ•°æ®ï¼ˆ$\pi</em>{old}$ æ”¶é›†åˆ°çš„æ ·æœ¬ï¼‰è¿›è¡Œç­–ç•¥æ›´æ–°ä¼šå¯¼è‡´ <strong>è¿‡å¤§çš„ç­–ç•¥æ›´æ–°</strong>ã€‚
é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè¦ä½¿ç”¨ <strong>off-policy</strong> çš„æ–¹æ³•å‘¢ï¼Ÿ
è¿™æ˜¯å› ä¸º <strong>data efficiency</strong> çš„ç¼˜æ•…ï¼Œon-policyæ–¹æ³•å¯¹äºæ•°æ®çš„åˆ©ç”¨æ•ˆç‡å¤ªä½ã€‚</p>

<p>è€Œ <strong>Trust-Region Policy Optimization</strong> é€šè¿‡æ•°å­¦æ¨å¯¼æå‡ºäº†ä¸€ç§ <strong>surrogate objective</strong>ï¼Œè¿™ç§ç›®æ ‡å‡½æ•°åœ¨å¯¹ç­–ç•¥æ›´æ–°å¹…åº¦ä¸ŠåŠ ä»¥é™åˆ¶åï¼Œå¯ä»¥åšåˆ°ç­–ç•¥çš„å•è°ƒæå‡ã€‚
å…¶ç›®æ ‡å‡½æ•°ä¸º 
\(\begin{align}
\mathop{maximize}\limits_{\theta} \quad \hat{\mathbb{E}}_t [\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A}_t]
\end{align}\)</p>

\[\begin{align}
subject\ to\ \hat{\mathbb{E}}_t [KL[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]] \leq \delta
\end{align}\]

<p>è¿™æ˜¯ä¸€ä¸ªå¸¦çº¦æŸçš„ç›®æ ‡å‡½æ•°ï¼Œæ ¹æ®æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•å¯å°†å¸¦çº¦æŸçš„å‡½æ•°è½¬æ¢ä¸ºå¸¦æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼ˆæƒ©ç½šï¼‰çš„ç›®æ ‡å‡½æ•°ï¼š
\(\begin{align}
\mathop{maximize}\limits_{\theta} \quad \hat{\mathbb{E}}_t [\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t - \beta KL[\pi_{\theta_{old}}(\cdot|s_t), \pi_{\theta}(\cdot|s_t)]]
\end{align}\)</p>

<p>ä½†æ˜¯åœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬æ— æ³•ç¡®å®šä¸€ä¸ªå›ºå®šçš„ $\beta$ æ¥é€‚åº”äºä¸åŒçš„ä»»åŠ¡ï¼Œç”šè‡³æ˜¯åŒä¸€ç§ä»»åŠ¡çš„ä¸åŒçŠ¶æ€ã€‚
ä½†æ˜¯ï¼ŒTRPOåˆæ‹¥æœ‰ä¸å¯å¤šå¾—çš„ä¼˜åŠ¿ï¼šå¯ä»¥guaranteeç­–ç•¥çš„ç¨³æ­¥ä¸Šå‡ï¼Œæˆ–è€…è‡³å°‘ä¸ä¼šä¸‹é™ã€‚</p>

<p>åŸºäºæ­¤ï¼Œæå‡ºäº†PPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹ <strong>probability ratio</strong> åŠ ä»¥æˆªæ–­ï¼Œå³é™åˆ¶ $r_t(\theta)$ çš„å¤§å°ï¼Œå½“ $r_t(\theta)$ ä½¿å¾—ç›®æ ‡å‡½æ•°å‡å°‘æ—¶ï¼Œæˆªæ–­å®ƒï¼Œä½¿å¾—ç›®æ ‡å‡½æ•°ä¸ä¼šå‡å°‘ã€‚
é€šè¿‡æˆªæ–­çš„æ–¹å¼è¾¾åˆ°ä¸TRPOç›¸åŒçš„æ•ˆæœï¼Œä½†æ˜¯å´æ¯”TRPOè¦æ›´ç®€å•ï¼Œå› ä¸ºå®ƒæ˜¯ä¼˜åŒ–ä¸€ä¸ªæ— çº¦æŸçš„ç›®æ ‡å‡½æ•°ã€‚
å…¶ç›®æ ‡å‡½æ•°ä¸ºï¼š
\(\begin{align}
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t)] \tag{7}
\end{align}\)
æ–‡ç« ä¸­é€šè¿‡å®éªŒè¯æ˜ï¼Œå½“ $\epsilon = 0.2$ æ—¶ï¼Œæ¨¡å‹æ€§èƒ½æœ€ä½³ã€‚</p>

<h3 id="æ¨¡å‹æµç¨‹å›¾">æ¨¡å‹æµç¨‹å›¾</h3>
<p>åŸºäºDeepMindçš„baselinesï¼Œç”»å‡ºäº†å…¶PPO2ç®—æ³•çš„æµç¨‹å›¾ï¼Œå¦‚ä¸‹ï¼š<br />
<img src="../image/ppoç½‘ç»œç»“æ„å›¾.png" alt="Figure 1" title="ppo structure" /></p>
:ET