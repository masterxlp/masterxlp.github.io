I"Y<ul id="markdown-toc">
  <li><a href="#简介" id="markdown-toc-简介">简介</a></li>
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
</ul>

<p>转载自<a href="https://zhuanlan.zhihu.com/p/58226117">知乎</a></p>

<h2 id="简介">简介</h2>
<blockquote>
  <p>Title: IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures<br />
Author: DeepMind<br />
Published Time: 2018<br />
Linked: <a href="https://arxiv.org/abs/1802.01561">IMPALA</a></p>
</blockquote>

<h2 id="abstract">Abstract</h2>
<blockquote>
  <p>这篇文章提出了一个可用于大规模强化学习训练的框架 – IMPALA，其主要思想是通过大量的与训练无关的Actor来采集样本，将训练集中于Learner中，这样采样与训练便可以并行运行，
该框架具有较高的性能、较好的扩展性以及较高的数据效率。</p>
</blockquote>

<blockquote>
  <p>V-Trace 技术则是用来矫正由于采样与训练并行时导致的策略错位情况，通过V-Trace技术完成使用Off-Policy的样本进行训练的目的。</p>
</blockquote>
:ET