I"7A<h2 id="c-多目标强化学习总结">[C] 多目标强化学习总结</h2>
<h3 id="wuji-automatic-online-combat-game-testing-using-evolutionary-deep-reinforcement-learning">Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning</h3>
<blockquote>
  <p>2019年 - 34th IEEE/ACM International Conference on Automated Software Engineering - 软件工程领域顶级会议论文<br />
Author: 天津大学强化学习实验室、网易伏羲人工智能实验室、NTU<br />
Link: <a href="https://yanzzzzz.github.io/files/PID6139619.pdf">原文链接</a><br />
参考自: <a href="https://www.jiqizhixin.com/articles/2019-10-22-11">机器之心</a></p>
</blockquote>

<h4 id="背景">背景</h4>
<p>如今在开发的各种游戏中存在各种各样的游戏缺陷（BUG），为了发现这些BUG游戏测试应运而生。在工业界，如今的游戏测试一般都还表现脚本测试以及手工测试相结合的形式。
时至今日，自动化游戏测试的研究仍然处于初级阶段，一个主要原因是玩游戏本身是一个持续决策的过程，而BUG往往隐藏的较深，只有当某些困难的中间任务完成后，
才有可能被触发，这就要求游戏测试算法拥有类人的智能。
近年来，深度强化学习算法（DRL）取得的非凡的成功，特别在游戏控制领域，甚至表现出了超越人类的智能，这为推进自动化游戏测试提供了启示。
然而，既有的 DRL 算法主要关注如何赢得游戏，而不是游戏测试，导致其可能无法广泛地覆盖需要测试的分支场景。</p>
<div align="center"><img src="../image/Wuji中智能体与环境的交互示意图.png" width="70%" height="50%" /></div>

<h4 id="wuji优势概括">wuji优势概括</h4>
<p>Wuji融合了进化算法、深度强化学习算法以及多目标优化机制，实现了智能的自动化游戏测试，在赢得游戏和探索游戏空间（两个目标）之间取得了较好的平衡，
其中，赢得游戏可以使得智能体在游戏中取得进展，而空间探索则可以增加发现BUG的可能性。</p>

<p><img src="../image/Wuji结构图.png" alt="Figure 2" title="Wuji结构图" /></p>

<h4 id="wuji---基于多目标优化的进化强化学习框架">Wuji - 基于多目标优化的进化强化学习框架</h4>
<p>从强化学习算法的角度看，不同的策略 $\pi$ 都能够探索到游戏中不同的状态空间。
从进化算法的角度看，通过维护一个策略“种群”，可以实现游戏空间的搞笑探索。
直观上，可以将二者结合，实现有效的游戏测试。
Wuji正式构建在这样的强化学习架构之上的（见图3）。</p>

<p>然而，进化算法需要选择优质的后代，如前文所述，使用胜率作为策略的单一衡量指标会使得种群内的策略都趋同于取胜，无法探测到更广泛的游戏空间，降低游戏测试效果。
为此，Wuji使用 <code class="highlighter-rouge">多目标优化机制</code>，<strong>对每个策略分别从胜率以及空间探索能力两个维度衡量策略性能</strong>，并以此进行优质后代的选择。</p>

<p>具体来说，每个策略用于后代选择的 <em>Fitness-value (FV)</em> 计算方式如下：</p>

\[\begin{align}
FV(\pi) = [RS_G^{\pi}, ES_G^{\pi}]
\end{align}\]

<p>例如，给定游戏 $G$，使用策略 $\pi$ 执行一个回合后，$RS_G^{\pi}$ 表示策略在当前回合的胜率，$ES_G^{\pi}$ 表示策略在当前回合中探索状态空间的数量。
至此，策略的 $FV$ 从标量扩展到了向量。
因此，后代选择的方式也从选择较大的标量转变为了 <strong>向量之间的比较</strong>。</p>

<p>在文章中，作者提出使用非支配排序（non-dominate sorting，NDS）来选择非支配集（non-dominate set），进而选择更优质的后代。
具体过程见图3(右)所示，图中每一个点表示一个策略，两个维度衡量了策略在获胜能力和探索能力两个维度上的表现。
在整个种群中存在一个集合 $F_1$，该集合中的策略相互不支配（例如 $\pi_1$ 的胜率比 $\pi_2$ 高，但是探索能力相比来说要低，因此，无法说明这两个策略谁更优秀），
该策略集又被称为帕累托前沿(Pareto Frontier)。</p>

<p>基于此，进行后代选择的时候，优先选择集合中的帕累托前沿 $F_1$，接着从种群中剔除 $F_1$ 后再进行非支配集的筛选，找到第二个帕累托前沿 $F_2$，再加入到后代中，
循环往复直到种群数量达到上限。
值得注意的是，当将 $F_3$ 加入到后代种群中时，如果遇到种群规模超出上限阈值的情况，需要针对 $F_3$ 内的策略进行筛选。
具体见算法(2-4)</p>

<div align="center"><img src="../image/wuji算法1.png" width="50%" height="50%" /></div>

<p>为此，提出使用 <strong>聚集距离（crowding distance）对策略的密集程度进行度量</strong>，
并基于聚集距离实现策略的聚集距离排序算法（Crowding distance sorting，CDS）实现策略的末位淘汰。
例如，如图3(b)所示，针对策略 $\pi_1$ 的聚集距离定义如下：</p>

\[\begin{align}
CD(\pi_1) = d_1 + d_2 + d_3 + d_4
\end{align}\]

<p>其中 $d_1$ 和 $d_4$ 衡量了在探索能力的维度上，距离 $\pi_1$ 最近的邻策略的距离，$d_2$ 和 $d_3$ 衡量了胜率维度的距离。
根据聚集距离对策略进行CDS，保留聚集距离较大的策略，淘汰聚集距离较小的策略，以此实现策略的多样性。
CDS尽可能选择两端的策略，以及均匀分布在两个极端之间的策略，以实现后代策略的多样性。</p>

<div align="center"><img src="../image/wuji算法2.png" width="50%" height="50%" /></div>

<p>综上所述，Wuji 借助进化强化学习算法框架，结合多目标优化机制，使得种群内的策略朝着胜率以及探索能力两个方向不断优化，
同时还保证部分策略均匀的分布在两个优化目标之间。二者的融合使 Wuji 能够完成更多任务并探索游戏的更多状态，提升发现 bug 的几率。</p>

<h3 id="direct-feature-prediction">Direct Feature Prediction</h3>
<blockquote>
  <p>Author1: Arthur Juliani<br />
Author2: 王瀚宸<br />
Link1: <a href="https://arxiv.org/abs/1611.01779">原文链接</a><br />
参考自<a href="https://mp.weixin.qq.com/s/XHdaoOWBgOWX7SrOemY4jw">量子位</a></p>
</blockquote>

<p>强化学习的主要内容就是不断的训练agent完成任务，我们认为这会让agent学会做这件事。
举例来说，假如我们希望训练一个会开门的机器人，或者叫agent，以强化学习为框架，就可以让机器人在不断试错中学会开门。</p>

<p>但如果我们希望agent能够完成多个目标呢，比如完成的目标需要随着时间变化而变化，那这时需要怎样做呢？</p>

<h4 id="q-learning--为了最大的累积奖励">Q-learning : 为了最大的累积奖励</h4>
<p>强化学习就是让agent在某个环境中不断的依据某种策略进行互动，其目标就是让agent随着时间的推移获得最大累积奖励。
这个过程往往以如下形式进行：一个agent从环境中接收到一个状态 $s \in \mathcal{S}$，进而依据策略 $\pi$ 产生相应的动作 $a \in \mathcal{A}$，
执行该动作，agent接收一个即时奖励 $r \in \mathbb{R}$ 以及下一个状态 $s’ \in \mathcal{S}$。
强化学习解决的问题就是学习一个从状态到动作的映射，即策略 $\pi$，以保证能够产生最大累积奖励。</p>

<p>Q-learning就是用来解决这类问题的一种方法：它学习的是状态-动作对与价值的估计值 $V$ 之间的直接关系$。
该估计值与在状态 $s$ 下采取动作 $a$ 所得到的期望累积折扣奖励相对应。
集合贝尔曼方程，我们可以通过迭代得到所有可能的“状态-动作对”的 $Q$ 估计值。
这种Q值迭代的方法来自于优化Q方程的一下性质：</p>

\[\begin{align}
Q^{*}(s,a) = r + \gamma \mathop{max}\limits_{a'}Q(s',a') \tag{1}
\end{align}\]

<p>上式的含义为：对于给定的状态 $s$ 以及动作 $a$，其当前的Q值可以分解为当前获得的即时奖励 $r$ 与下一状态的期望累积折扣奖励的和。
通过收集experience，我们训练的神经网络能够随着时间的推移越来越准确的估计真实Q值。，随后通过采取最优值的动作，理论上既能够从环境中获得最大的累积奖励。</p>

<p>使用一个诸如神经网络的全局功能近似器，我们能够对未发生的状态归纳出其最接近真实Q值的Q的估计值，从而使我们能够了解任意大的状态空间下的Q方程。</p>

<h4 id="无人机送货--基于目标的强化学习">无人机送货 &amp; 基于目标的强化学习</h4>
<p>Q-learning和其他传统的强化学习算法都采用单一奖励信号，因此也只能完成单一目标。</p>

<p>在我们设置的环境中，agent将占据 5 * 5 方格拦中的一个位置，然后目的地在另外一个位置，这个agent可以在上下左右四个方向上随意移动。
如果我们希望无人机能够学会运送货物，我们通常会在无人机成功飞到标记点、完成送货时，提供一个+1的正向奖励。</p>

<div align="center"><img src="../image/无人机送货示意图.png" width="50%" height="50%" /></div>

<p>上图展示了我们agent的学习内容和环境，我们将会使用 5 * 5 的RGB网格(共有75种可能)这样一个更简单的形式表示环境。
这会把学习的时间从小时量级降低到分钟量级。
每个training episode中agent可以移动100步，在每个episode开始之前会随机分配agent和目的地的位置。</p>

<h4 id="使用tensorflow实践q-learning">使用Tensorflow实践Q-learning</h4>
<p>一下所示的使用Tensorflow执行Q-learning算法的异步版本，即通过同时运行多个agent来学习策略，这能在加速训练过程中同时增加稳定性。
具体实现过程见：<a href="https://github.com/awjuliani/dfp/blob/master/Async-Q.ipynb">链接</a></p>

<p>我们在一台机器上训练四个worker，在每个worker经历过6000个training episode后，我们得到了一个类似下图的性能曲线。</p>

<p>我们的agent能够在每个episode中完成20次投递，这个值可以被认为是在当前场景下的单个episode中的最大可能投递数。</p>

<h4 id="直接特征预测">直接特征预测</h4>
<p>因为电池容量有限，需要经常充电，所以在真实世界中的无人机并不能一直送货。
无人机的每一步都会消耗电磁中的一部分电量，而一旦电量用完，就意味着无人机将会从空中坠落，无法继续送货，在程序中即象征着没有更多的奖励。</p>

<h5 id="建立一个优化的奖励函数">建立一个优化的奖励函数</h5>
<p>在时间充足、超参数经过恰当调优的情况下，Q-learning算法最终能够发现，适当进行充电在长期看来有助于配送更多包裹。
这样，agent就学会了在没有短期奖励的情况下，采取一系列复杂的行动，从而获得长期更大的奖励。</p>

<p>在这种情景下，设置一个奖励信号用来鼓励无人机进行充电乍一看好像不错。
一个naive的想法就是当无人机向特定地点充电时提供一个奖励（比如+0.5）。
但在这种情况下，我们的agent将学会的行为就是只会不停的飞向充电站，因为那里肯定有奖励。</p>

<p>而我们需要做的，是构造出一个描述最优化行为的奖励函数，这个过程对一些问题来说是很简单的，因为不恰当的奖励通常会导致agent出乎意料的行为。</p>

<h5 id="改变目标">改变目标</h5>
<p>如果希望避免由不正确的奖励函数导致的错误，我们就需要将任务形式以更直观的方式转述给agent。
我们发现，<strong>基于episode和特定时刻提供给agent一个明确的目标，能够更好的优化agent的行为</strong>。</p>

<p>比如说，针对充电的问题，我们可以在无人机电量低于一个阈值的时候将行动目标从“送包裹”改为“去充电”。
这样，我们就不用担心奖励函数的形式，而且神经网络也可以专心学习环境的动态变化。</p>

<h5 id="将寻找目标的任务形式化">将寻找目标的任务形式化</h5>
<p>为了让这个概念便于使用，我们需要对上述描述正式化。</p>

<p>在强化学习中有多招方式可以实现目标寻找，我们接下来介绍一种来自于ICLR回忆上的一篇文章“Learning Act by Predicting the Future”，
这正是我们准备训练我们的agent做的事情。</p>

<p>首先说明，文中展示的模型并不是论文结果的直接应用。
在论文中，他们把网络称为“直接特征预测（direct feature prediction，DFP）”。
而我们所做的是类似DFP的一个简化版本。
我们做了一些调整，从而使例子更加直观。</p>

<p>在原论文中，作者训练他们的agent在第一人称射击游戏“Doom”中进行对抗，令人印象深刻，但是对于文本来说难度太大。</p>

<div align="center"><img src="../image/measurements.png" />&gt;</div>

<p>与一般的agent训练的过程不同，我们不是去建立状态 $s$ 和Q值估计值 $Q(s,a)$ 之间的映射，然后从环境中获得一个奖励 $r$，
而是对状态 $s$ 增加了一系列测量值 $m$ 和目标值 $g$，然后训练神经网络对每个动作 $a$ 去预测未来策略值的改变 $f$。</p>

<h5 id="训练网络预测未来的期望">训练网络预测未来的期望</h5>
<p>在无人机送货的场景中，我们将会用到两个测量值：电池电量和成功配送的包裹数。</p>

<p>与在Q-learning中预测一个值函数不同，我们训练网络去预测在未来的1，2，4，8，16和32步移动之后电池的电量和配送的包裹数。
形式上这可以写成：</p>

\[\begin{align}
f = &lt;m_{T_1} - m_0, m_{T_2} - m_0, \cdots, m_{T_n} - m_0&gt; \tag{2}
\end{align}\]

<p>这里的 $T_i$ 代表时间偏移量的列表，即 $[1, 2, 4, \cdots]$，表示agent移动的步数。</p>

<p>在这个例子中，不再有明确的奖励，取而代之的是用目标和测量值的匹配程度作为成功的衡量。
在这个无人机配送的例子中，具体含义就是要最大化送货量的同时保证低电量时给电池充电。</p>

<p>如果我们的agent能够完美的预测未来每一步的测量值，我们就只需要采用能使测量值最优化的行动策略。
我们的目标允许我们在任意时间下指定在意的测量值。</p>

<h5 id="对更复杂目标的规划">对更复杂目标的规划</h5>
<p>因为我们并不像Q-learning中那样简单的预测一个标量的估计值，就可以对更复杂的目标进行规划。
假设我们构造一个测量值矢量 $[电量, 配送数]$，如果我们希望最大化电池电量的同时忽略配送数，那么我们的目标就是 $[1, 0]$，即意味着希望未来有一个正向的电量测量值的同时忽略配送数。
如果我们希望最大化包裹的配送数，那么目标就是 $[0, 1]$。</p>

<p>目标是由我们制定的，而不是由环境或者网络决定的，因此在任何合适的时间步我们都可以改变它。</p>

<p>这样，我们就可以在电量低于一个阈值时，把目标从“最大化配送数”改为“最大化电量”。
通过将目标和测量值以这种方式结合，我们现在可以灵活的调整agent在飞行过程中的行为。</p>

<p>这种做法与Q-learning是相冲突的，在Q-learning中Q值在训练后保持不变，因此只会有一种行为模式。
同时，这个新模式从多个方面改变了我们的神经网络。
现在，网络的输入不再是状态 $s$，而是测量值 $m$ 和目标 $g$；之前是输出Q值，现在我们的网络是输入一个形式为 $[测量值 \times 行动 \times 偏移]$ 的预测张量。
将总计预测未来改变和目标相乘，可以选择出随时间变化能更好满足目标的行动：</p>

\[\begin{aling}
a = g^T \sum p(s,m,g) \tag{3}
\end{align}\]

<p>其中，$\sum p(s,m,g)$ 表示网络随未来时序的总和，$g^T$ 是目标矢量的转置。</p>

<p>我们可以使用简单的衰退损失来训练agent预测未来真实测量值的变化：</p>

\[\begin{align}
loss = \sum [P[s,m,g,a] - f(m)]^2 \tag{4}
\end{align}\]

<p>其中，$P(s,m,g,a$ 表示对于选定的行动网络的输出结果。</p>

<p>当把这些结合起来时，我们就得到了一个符合要求的agent。</p>
:ET