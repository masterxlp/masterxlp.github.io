I"H<h2 id="c-多目标强化学习总结">[C] 多目标强化学习总结</h2>
<h3 id="wuji-automatic-online-combat-game-testing-using-evolutionary-deep-reinforcement-learning">Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning</h3>
<blockquote>
  <p>2019年 - 34th IEEE/ACM International Conference on Automated Software Engineering - 软件工程领域顶级会议论文<br />
Author: 天津大学强化学习实验室、网易伏羲人工智能实验室、NTU<br />
Link: <a href="https://yanzzzzz.github.io/files/PID6139619.pdf">原文链接</a><br />
参考自: <a href="https://www.jiqizhixin.com/articles/2019-10-22-11">机器之心</a></p>
</blockquote>

<h4 id="背景">背景</h4>
<p>如今在开发的各种游戏中存在各种各样的游戏缺陷（BUG），为了发现这些BUG游戏测试应运而生。在工业界，如今的游戏测试一般都还表现脚本测试以及手工测试相结合的形式。
时至今日，自动化游戏测试的研究仍然处于初级阶段，一个主要原因是玩游戏本身是一个持续决策的过程，而BUG往往隐藏的较深，只有当某些困难的中间任务完成后，
才有可能被触发，这就要求游戏测试算法拥有类人的智能。
近年来，深度强化学习算法（DRL）取得的非凡的成功，特别在游戏控制领域，甚至表现出了超越人类的智能，这为推进自动化游戏测试提供了启示。
然而，既有的 DRL 算法主要关注如何赢得游戏，而不是游戏测试，导致其可能无法广泛地覆盖需要测试的分支场景。
<img src="../image/Wuji中智能体与环境的交互示意图.png" alt="Figure 1" title="Wuji中智能体与环境的交互示意图" /></p>

<h4 id="wuji优势概括">wuji优势概括</h4>
<p>Wuji融合了进化算法、深度强化学习算法以及多目标优化机制，实现了智能的自动化游戏测试，在赢得游戏和探索游戏空间（两个目标）之间取得了较好的平衡，
其中，赢得游戏可以使得智能体在游戏中取得进展，而空间探索则可以增加发现BUG的可能性。</p>

<p><img src="../image/Wuji结构图.png" alt="Figure 2" title="Wuji结构图" /></p>

<h4 id="wuji---基于多目标优化的进化强化学习框架">Wuji - 基于多目标优化的进化强化学习框架</h4>
<p>从强化学习算法的角度看，不同的策略 $\pi$ 都能够探索到游戏中不同的状态空间。
从进化算法的角度看，通过维护一个策略“种群”，可以实现游戏空间的搞笑探索。
直观上，可以将二者结合，实现有效的游戏测试。
Wuji正式构建在这样的强化学习架构之上的（见图3）。</p>

<p>然而，进化算法需要选择优质的后代，如前文所述，使用胜率作为策略的单一衡量指标会使得种群内的策略都趋同于取胜，无法探测到更广泛的游戏空间，降低游戏测试效果。
为此，Wuji使用 <code class="highlighter-rouge">多目标优化机制</code>，<strong>对每个策略分别从胜率以及空间探索能力两个维度衡量策略性能</strong>，并以此进行优质后代的选择。</p>

<p>具体来说，每个策略用于后代选择的 <em>Fitness-value (FV)</em> 计算方式如下：</p>

\[\begin{align}
FV(\pi) = [RS_G^{\pi}, ES_G^{\pi}]
\end{align}\]

<p>例如，给定游戏 $G$，使用策略 $\pi$ 执行一个回合后，$RS_G^{\pi}$ 表示策略在当前回合的胜率，$ES_G^{\pi}$ 表示策略在当前回合中探索状态空间的数量。
至此，策略的 $FV$ 从标量扩展到了向量。
因此，后代选择的方式也从选择较大的标量转变为了 <strong>向量之间的比较</strong>。</p>

<p>在文章中，作者提出使用非支配排序（non-dominate sorting，NDS）来选择非支配集（non-dominate set），进而选择更优质的后代。
具体过程见图3(右)所示，图中每一个点表示一个策略，两个维度衡量了策略在获胜能力和探索能力两个维度上的表现。
在整个种群中存在一个集合 $F_1$，该集合中的策略相互不支配（例如 $\pi_1$ 的胜率比 $\pi_2$ 高，但是探索能力相比来说要低，因此，无法说明这两个策略谁更优秀），
该策略集又被称为帕累托前沿(Pareto Frontier)。</p>

<p>基于此，进行后代选择的时候，优先选择集合中的帕累托前沿 $F_1$，接着从种群中剔除 $F_1$ 后再进行非支配集的筛选，找到第二个帕累托前沿 $F_2$，再加入到后代中，
循环往复直到种群数量达到上限。
值得注意的是，当将 $F_3$ 加入到后代种群中时，如果遇到种群规模超出上限阈值的情况，需要针对 $F_3$ 内的策略进行筛选。
具体见算法(2-4)</p>

<p><img src="../image/Wuji算法1.png" alt="Figure 3" title="Wuji算法1" /></p>

<p>为此，提出使用 <strong>聚集距离（crowding distance）对策略的密集程度进行度量</strong>，
并基于聚集距离实现策略的聚集距离排序算法（Crowding distance sorting，CDS）实现策略的末位淘汰。
例如，如图3(b)所示，针对策略 $\pi_1$ 的聚集距离定义如下：</p>

\[\begin{align}
CD(\pi_1) = d_1 + d_2 + d_3 + d_4
\end{align}\]

<p>其中 $d_1$ 和 $d_4$ 衡量了在探索能力的维度上，距离 $\pi_1$ 最近的邻策略的距离，$d_2$ 和 $d_3$ 衡量了胜率维度的距离。
根据聚集距离对策略进行CDS，保留聚集距离较大的策略，淘汰聚集距离较小的策略，以此实现策略的多样性。
CDS尽可能选择两端的策略，以及均匀分布在两个极端之间的策略，以实现后代策略的多样性。</p>

<p><img src="../image/Wuji算法2.png" alt="Figure 4" title="Wuji算法2" /></p>

<p>综上所述，Wuji 借助进化强化学习算法框架，结合多目标优化机制，使得种群内的策略朝着胜率以及探索能力两个方向不断优化，
同时还保证部分策略均匀的分布在两个优化目标之间。二者的融合使 Wuji 能够完成更多任务并探索游戏的更多状态，提升发现 bug 的几率。</p>

<h3 id="direct-feature-prediction">Direct Feature Prediction</h3>
<blockquote>
  <p>Author1: Arthur Juliani<br />
Author2: 王瀚宸<br />
Link1: <a href="https://arxiv.org/abs/1611.01779">原文链接</a><br />
参考自<a href="https://mp.weixin.qq.com/s/XHdaoOWBgOWX7SrOemY4jw">量子位</a></p>
</blockquote>
:ET